{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d8854f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "057d188f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, train_loss: 0.22626051844193554 \n",
      "epoch: 2, train_loss: 0.1050262300178331 \n",
      "epoch: 3, train_loss: 0.10620681933950785 \n",
      "epoch: 4, train_loss: 0.10632052427397443 \n",
      "epoch: 5, train_loss: 0.10641957331582674 \n",
      "epoch: 6, train_loss: 0.10653670465737333 \n",
      "epoch: 7, train_loss: 0.10662527226502118 \n",
      "epoch: 8, train_loss: 0.1140286558871945 \n",
      "epoch: 9, train_loss: 0.12152978615969742 \n",
      "epoch: 10, train_loss: 0.1221209389388869 \n",
      "epoch: 11, train_loss: 0.1223188582141961 \n",
      "epoch: 12, train_loss: 0.12244406991688736 \n",
      "epoch: 13, train_loss: 0.12237083505634269 \n",
      "epoch: 14, train_loss: 0.12205756996031505 \n",
      "epoch: 15, train_loss: 0.12207480226782644 \n",
      "epoch: 16, train_loss: 0.12204248514629093 \n",
      "epoch: 17, train_loss: 0.12224394940823462 \n",
      "epoch: 18, train_loss: 0.1231162998475159 \n",
      "epoch: 19, train_loss: 0.12314005707159026 \n",
      "epoch: 20, train_loss: 0.12316086421312143 \n",
      "epoch: 21, train_loss: 0.12317958325224901 \n",
      "epoch: 22, train_loss: 0.12319662276078414 \n",
      "epoch: 23, train_loss: 0.12321228220022616 \n",
      "epoch: 24, train_loss: 0.12322677931115066 \n",
      "epoch: 25, train_loss: 0.12324027213223145 \n",
      "epoch: 26, train_loss: 0.12325287760414025 \n",
      "epoch: 27, train_loss: 0.12326468636881804 \n",
      "epoch: 28, train_loss: 0.12327577363255685 \n",
      "epoch: 29, train_loss: 0.12328620605055171 \n",
      "epoch: 30, train_loss: 0.12329604517520996 \n",
      "epoch: 31, train_loss: 0.12330534854659093 \n",
      "epoch: 32, train_loss: 0.12331416958743655 \n",
      "epoch: 33, train_loss: 0.12332255716648241 \n",
      "epoch: 34, train_loss: 0.1233305552881894 \n",
      "epoch: 35, train_loss: 0.12333820305384549 \n",
      "epoch: 36, train_loss: 0.12334553486995777 \n",
      "epoch: 37, train_loss: 0.12335258082081987 \n",
      "epoch: 38, train_loss: 0.12335936712179132 \n",
      "epoch: 39, train_loss: 0.12336591659094724 \n",
      "epoch: 40, train_loss: 0.12337224910013152 \n",
      "epoch: 41, train_loss: 0.12337838198490157 \n",
      "epoch: 42, train_loss: 0.12338433040524704 \n",
      "epoch: 43, train_loss: 0.12339010765635561 \n",
      "epoch: 44, train_loss: 0.12339572543262636 \n",
      "epoch: 45, train_loss: 0.12340119404984945 \n",
      "epoch: 46, train_loss: 0.1234065226309302 \n",
      "epoch: 47, train_loss: 0.12341171926029094 \n",
      "epoch: 48, train_loss: 0.12341679111153853 \n",
      "epoch: 49, train_loss: 0.12342174455228759 \n",
      "epoch: 50, train_loss: 0.12342658522932318 \n",
      "epoch: 51, train_loss: 0.12343131813660643 \n",
      "epoch: 52, train_loss: 0.12343594766795717 \n",
      "epoch: 53, train_loss: 0.1234404776556323 \n",
      "epoch: 54, train_loss: 0.12344491139536246 \n",
      "epoch: 55, train_loss: 0.1234492516577389 \n",
      "epoch: 56, train_loss: 0.12345350068504654 \n",
      "epoch: 57, train_loss: 0.12345766017167004 \n",
      "epoch: 58, train_loss: 0.12346173122491907 \n",
      "epoch: 59, train_loss: 0.12346571430135182 \n",
      "epoch: 60, train_loss: 0.12346960911110377 \n",
      "epoch: 61, train_loss: 0.12347341447892747 \n",
      "epoch: 62, train_loss: 0.12347712814474238 \n",
      "epoch: 63, train_loss: 0.12348074647717923 \n",
      "epoch: 64, train_loss: 0.12348426405836509 \n",
      "epoch: 65, train_loss: 0.1234876730725559 \n",
      "epoch: 66, train_loss: 0.12349096238650305 \n",
      "epoch: 67, train_loss: 0.12349411612838199 \n",
      "epoch: 68, train_loss: 0.12349711141844062 \n",
      "epoch: 69, train_loss: 0.12349991459756467 \n",
      "epoch: 70, train_loss: 0.12350247464760064 \n",
      "epoch: 71, train_loss: 0.12350471100291707 \n",
      "epoch: 72, train_loss: 0.12350648919597094 \n",
      "epoch: 73, train_loss: 0.1235075671414828 \n",
      "epoch: 74, train_loss: 0.1235074595266922 \n",
      "epoch: 75, train_loss: 0.12350502009348535 \n",
      "epoch: 76, train_loss: 0.12349664199011275 \n",
      "epoch: 77, train_loss: 0.12345834738094642 \n",
      "epoch: 78, train_loss: 0.1250261366526995 \n",
      "epoch: 79, train_loss: 0.12644767368987947 \n",
      "epoch: 80, train_loss: 0.12644188562716596 \n",
      "epoch: 81, train_loss: 0.12644004956276286 \n",
      "epoch: 82, train_loss: 0.12643983649891322 \n",
      "epoch: 83, train_loss: 0.12644040885779662 \n",
      "epoch: 84, train_loss: 0.126441402519196 \n",
      "epoch: 85, train_loss: 0.12644263871961184 \n",
      "epoch: 86, train_loss: 0.12644402149072123 \n",
      "epoch: 87, train_loss: 0.12644549565778504 \n",
      "epoch: 88, train_loss: 0.12644702775537067 \n",
      "epoch: 89, train_loss: 0.12644859661719662 \n",
      "epoch: 90, train_loss: 0.12645018841916897 \n",
      "epoch: 91, train_loss: 0.1264517939225342 \n",
      "epoch: 92, train_loss: 0.12645340687021714 \n",
      "epoch: 93, train_loss: 0.12645502301907166 \n",
      "epoch: 94, train_loss: 0.1264566395390203 \n",
      "epoch: 95, train_loss: 0.12645825463310137 \n",
      "epoch: 96, train_loss: 0.12645986729654216 \n",
      "epoch: 97, train_loss: 0.1264614771679947 \n",
      "epoch: 98, train_loss: 0.12646308444620774 \n",
      "epoch: 99, train_loss: 0.12646468985788512 \n",
      "epoch: 100, train_loss: 0.12646629467106588 \n",
      "epoch: 101, train_loss: 0.12646790075551512 \n",
      "epoch: 102, train_loss: 0.12646951069907056 \n",
      "epoch: 103, train_loss: 0.12647112799863508 \n",
      "epoch: 104, train_loss: 0.1264727573593339 \n",
      "epoch: 105, train_loss: 0.12647440516027236 \n",
      "epoch: 106, train_loss: 0.12647608019002454 \n",
      "epoch: 107, train_loss: 0.12647779483981475 \n",
      "epoch: 108, train_loss: 0.12647956711271172 \n",
      "epoch: 109, train_loss: 0.12648142417192415 \n",
      "epoch: 110, train_loss: 0.12648340899386673 \n",
      "epoch: 111, train_loss: 0.1264855938297235 \n",
      "epoch: 112, train_loss: 0.1264881102978542 \n",
      "epoch: 113, train_loss: 0.12649122651356753 \n",
      "epoch: 114, train_loss: 0.12649558915208692 \n",
      "epoch: 115, train_loss: 0.12650329531747107 \n",
      "epoch: 116, train_loss: 0.1265291205305383 \n",
      "epoch: 117, train_loss: 0.12602964494957822 \n",
      "epoch: 118, train_loss: 0.1250247511861298 \n",
      "epoch: 119, train_loss: 0.12501177390917112 \n",
      "epoch: 120, train_loss: 0.12500716120909647 \n",
      "epoch: 121, train_loss: 0.12500469214405915 \n",
      "epoch: 122, train_loss: 0.12500311886776427 \n",
      "epoch: 123, train_loss: 0.12500200931682334 \n",
      "epoch: 124, train_loss: 0.12500117159556684 \n",
      "epoch: 125, train_loss: 0.12500050665042986 \n",
      "epoch: 126, train_loss: 0.12499995781187392 \n",
      "epoch: 127, train_loss: 0.12499949009180862 \n",
      "epoch: 128, train_loss: 0.12499908055242284 \n",
      "epoch: 129, train_loss: 0.12499871338583982 \n",
      "epoch: 130, train_loss: 0.12499837720898165 \n",
      "epoch: 131, train_loss: 0.12499806348399893 \n",
      "epoch: 132, train_loss: 0.12499776554669911 \n",
      "epoch: 133, train_loss: 0.12499747797958237 \n",
      "epoch: 134, train_loss: 0.12499719618728536 \n",
      "epoch: 135, train_loss: 0.12499691609339157 \n",
      "epoch: 136, train_loss: 0.12499663390978792 \n",
      "epoch: 137, train_loss: 0.124996345947141 \n",
      "epoch: 138, train_loss: 0.12499604844436135 \n",
      "epoch: 139, train_loss: 0.12499573739936623 \n",
      "epoch: 140, train_loss: 0.124995408384533 \n",
      "epoch: 141, train_loss: 0.12499505632844131 \n",
      "epoch: 142, train_loss: 0.12499467524047815 \n",
      "epoch: 143, train_loss: 0.12499425784539642 \n",
      "epoch: 144, train_loss: 0.12499379507845453 \n",
      "epoch: 145, train_loss: 0.12499327536335378 \n",
      "epoch: 146, train_loss: 0.12499268354536996 \n",
      "epoch: 147, train_loss: 0.12499199926189311 \n",
      "epoch: 148, train_loss: 0.12499119436277742 \n",
      "epoch: 149, train_loss: 0.12499022865730858 \n",
      "epoch: 150, train_loss: 0.12498904256243529 \n",
      "epoch: 151, train_loss: 0.12498754365364033 \n",
      "epoch: 152, train_loss: 0.12498558028890597 \n",
      "epoch: 153, train_loss: 0.12498288513152518 \n",
      "epoch: 154, train_loss: 0.12497893950761685 \n",
      "epoch: 155, train_loss: 0.12497259225744364 \n",
      "epoch: 156, train_loss: 0.12496071446317712 \n",
      "epoch: 157, train_loss: 0.12493145964824373 \n",
      "epoch: 158, train_loss: 0.12482010173510887 \n",
      "epoch: 159, train_loss: 0.12449107630523931 \n",
      "epoch: 160, train_loss: 0.12390892772918627 \n",
      "epoch: 161, train_loss: 0.12340320401686575 \n",
      "epoch: 162, train_loss: 0.12319326036022679 \n",
      "epoch: 163, train_loss: 0.12310788744614545 \n",
      "epoch: 164, train_loss: 0.12306570084801548 \n",
      "epoch: 165, train_loss: 0.1230417305903826 \n",
      "epoch: 166, train_loss: 0.12302733296762092 \n",
      "epoch: 167, train_loss: 0.1230192716908464 \n",
      "epoch: 168, train_loss: 0.12301438793720643 \n",
      "epoch: 169, train_loss: 0.12301012025066403 \n",
      "epoch: 170, train_loss: 0.12300601907453021 \n",
      "epoch: 171, train_loss: 0.1230012638482258 \n",
      "epoch: 172, train_loss: 0.12299561880721674 \n",
      "epoch: 173, train_loss: 0.12298952611784808 \n",
      "epoch: 174, train_loss: 0.12298360343881361 \n",
      "epoch: 175, train_loss: 0.12297841394868499 \n",
      "epoch: 176, train_loss: 0.12297438826144075 \n",
      "epoch: 177, train_loss: 0.1229717840065654 \n",
      "epoch: 178, train_loss: 0.12297068959524417 \n",
      "epoch: 179, train_loss: 0.1229709016446468 \n",
      "epoch: 180, train_loss: 0.12297145775051001 \n",
      "epoch: 181, train_loss: 0.12297106852395091 \n",
      "epoch: 182, train_loss: 0.12296992265853875 \n",
      "epoch: 183, train_loss: 0.1229686136160222 \n",
      "epoch: 184, train_loss: 0.12296914923409537 \n",
      "epoch: 185, train_loss: 0.12296989400101101 \n",
      "epoch: 186, train_loss: 0.12296906065673811 \n",
      "epoch: 187, train_loss: 0.1229676803242631 \n",
      "epoch: 188, train_loss: 0.12296702011410525 \n",
      "epoch: 189, train_loss: 0.12296742030641793 \n",
      "epoch: 190, train_loss: 0.12296837281723758 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 191, train_loss: 0.1229680777422011 \n",
      "epoch: 192, train_loss: 0.12296728950957471 \n",
      "epoch: 193, train_loss: 0.12297146722138104 \n",
      "epoch: 194, train_loss: 0.12298081044543835 \n",
      "epoch: 195, train_loss: 0.12304146176029745 \n",
      "epoch: 196, train_loss: 0.12466250726425546 \n",
      "epoch: 197, train_loss: 0.12593397072809806 \n",
      "epoch: 198, train_loss: 0.12630065345817923 \n",
      "epoch: 199, train_loss: 0.1264357457248655 \n",
      "epoch: 200, train_loss: 0.12650107485374534 \n",
      "epoch: 201, train_loss: 0.1265277973203921 \n",
      "epoch: 202, train_loss: 0.12652114131209402 \n",
      "epoch: 203, train_loss: 0.12652730204228754 \n",
      "epoch: 204, train_loss: 0.126539565898978 \n",
      "epoch: 205, train_loss: 0.12653698853991902 \n",
      "epoch: 206, train_loss: 0.1265698302504743 \n",
      "epoch: 207, train_loss: 0.1265700588938871 \n",
      "epoch: 208, train_loss: 0.12662416804230017 \n",
      "epoch: 209, train_loss: 0.12663217585520392 \n",
      "epoch: 210, train_loss: 0.12663612622794523 \n",
      "epoch: 211, train_loss: 0.12662260857554555 \n",
      "epoch: 212, train_loss: 0.12659280587153574 \n",
      "epoch: 213, train_loss: 0.12658196047901665 \n",
      "epoch: 214, train_loss: 0.12656822277576638 \n",
      "epoch: 215, train_loss: 0.12656448666604053 \n",
      "epoch: 216, train_loss: 0.12656752109920444 \n",
      "epoch: 217, train_loss: 0.12656229684491657 \n",
      "epoch: 218, train_loss: 0.12655809789935307 \n",
      "epoch: 219, train_loss: 0.12655279367151684 \n",
      "epoch: 220, train_loss: 0.12654651811345405 \n",
      "epoch: 221, train_loss: 0.12653939568261668 \n",
      "epoch: 222, train_loss: 0.12653196189077695 \n",
      "epoch: 223, train_loss: 0.12652553807010197 \n",
      "epoch: 224, train_loss: 0.12652014164432182 \n",
      "epoch: 225, train_loss: 0.12651461089713253 \n",
      "epoch: 226, train_loss: 0.12650972561910373 \n",
      "epoch: 227, train_loss: 0.12652081444710012 \n",
      "epoch: 228, train_loss: 0.1265311259545644 \n",
      "epoch: 229, train_loss: 0.12654567506432143 \n",
      "epoch: 230, train_loss: 0.12943457610630554 \n",
      "epoch: 231, train_loss: 0.1323764918861081 \n",
      "epoch: 232, train_loss: 0.13317190192214615 \n",
      "epoch: 233, train_loss: 0.13356503896791075 \n",
      "epoch: 234, train_loss: 0.13377779897697864 \n",
      "epoch: 235, train_loss: 0.13396263790480692 \n",
      "epoch: 236, train_loss: 0.13402334923275822 \n",
      "epoch: 237, train_loss: 0.134048749803141 \n",
      "epoch: 238, train_loss: 0.13407275295604407 \n",
      "epoch: 239, train_loss: 0.1340815290254981 \n",
      "epoch: 240, train_loss: 0.13401431901417604 \n",
      "epoch: 241, train_loss: 0.13388544072930803 \n",
      "epoch: 242, train_loss: 0.13405610609547622 \n",
      "epoch: 243, train_loss: 0.13435006149890313 \n",
      "epoch: 244, train_loss: 0.13374305437683545 \n",
      "epoch: 245, train_loss: 0.13431235278037787 \n",
      "epoch: 246, train_loss: 0.13423451429003283 \n",
      "epoch: 247, train_loss: 0.1337820677133468 \n",
      "epoch: 248, train_loss: 0.134031943344078 \n",
      "epoch: 249, train_loss: 0.13402528509354364 \n",
      "epoch: 250, train_loss: 0.13380416381788188 \n",
      "epoch: 251, train_loss: 0.13390520198745484 \n",
      "epoch: 252, train_loss: 0.1340133433766657 \n",
      "epoch: 253, train_loss: 0.13369451317768147 \n",
      "epoch: 254, train_loss: 0.13372245743216526 \n",
      "epoch: 255, train_loss: 0.13369510445081018 \n",
      "epoch: 256, train_loss: 0.1336620060883618 \n",
      "epoch: 257, train_loss: 0.13409838120717898 \n",
      "epoch: 258, train_loss: 0.1339027877545307 \n",
      "epoch: 259, train_loss: 0.13400651365010902 \n",
      "epoch: 260, train_loss: 0.1337314679614985 \n",
      "epoch: 261, train_loss: 0.13366060117347367 \n",
      "epoch: 262, train_loss: 0.13366379124496217 \n",
      "epoch: 263, train_loss: 0.1336701083393182 \n",
      "epoch: 264, train_loss: 0.13367384390243084 \n",
      "epoch: 265, train_loss: 0.1336735671556094 \n",
      "epoch: 266, train_loss: 0.1336717893131731 \n",
      "epoch: 267, train_loss: 0.13366801564941613 \n",
      "epoch: 268, train_loss: 0.13358310742853227 \n",
      "epoch: 269, train_loss: 0.1334541460731359 \n",
      "epoch: 270, train_loss: 0.13391944104885906 \n",
      "epoch: 271, train_loss: 0.1338505722899008 \n",
      "epoch: 272, train_loss: 0.1336490916096534 \n",
      "epoch: 273, train_loss: 0.133634384313873 \n",
      "epoch: 274, train_loss: 0.1332009828489993 \n",
      "epoch: 275, train_loss: 0.13338293947339488 \n",
      "epoch: 276, train_loss: 0.1339818790834271 \n",
      "epoch: 277, train_loss: 0.1334921062508837 \n",
      "epoch: 278, train_loss: 0.1332735716536618 \n",
      "epoch: 279, train_loss: 0.13395704892216448 \n",
      "epoch: 280, train_loss: 0.13357796953054202 \n",
      "epoch: 281, train_loss: 0.13326390633297178 \n",
      "epoch: 282, train_loss: 0.1332080742107545 \n",
      "epoch: 283, train_loss: 0.13307436241157589 \n",
      "epoch: 284, train_loss: 0.13267628154562902 \n",
      "epoch: 285, train_loss: 0.13307189805150504 \n",
      "epoch: 286, train_loss: 0.13308901694004366 \n",
      "epoch: 287, train_loss: 0.13368660385748057 \n",
      "epoch: 288, train_loss: 0.13315728287565587 \n",
      "epoch: 289, train_loss: 0.13268613498951057 \n",
      "epoch: 290, train_loss: 0.13322351273261968 \n",
      "epoch: 291, train_loss: 0.13369638426675737 \n",
      "epoch: 292, train_loss: 0.1332780299619308 \n",
      "epoch: 293, train_loss: 0.13362046246248013 \n",
      "epoch: 294, train_loss: 0.13359402011096125 \n",
      "epoch: 295, train_loss: 0.13372600865908438 \n",
      "epoch: 296, train_loss: 0.1336899245844392 \n",
      "epoch: 297, train_loss: 0.1334412697759805 \n",
      "epoch: 298, train_loss: 0.1332745232141755 \n",
      "epoch: 299, train_loss: 0.13322015301014375 \n",
      "epoch: 300, train_loss: 0.13349603753711609 \n",
      "epoch: 301, train_loss: 0.1330052727230564 \n",
      "epoch: 302, train_loss: 0.1335888411048435 \n",
      "epoch: 303, train_loss: 0.13348103324327046 \n",
      "epoch: 304, train_loss: 0.1333410869451573 \n",
      "epoch: 305, train_loss: 0.13350053167384895 \n",
      "epoch: 306, train_loss: 0.1329594956207164 \n",
      "epoch: 307, train_loss: 0.13334742449677817 \n",
      "epoch: 308, train_loss: 0.13351911737485755 \n",
      "epoch: 309, train_loss: 0.1331477875088342 \n",
      "epoch: 310, train_loss: 0.1330644658584526 \n",
      "epoch: 311, train_loss: 0.13351841861128935 \n",
      "epoch: 312, train_loss: 0.13340905808996215 \n",
      "epoch: 313, train_loss: 0.1334393400632541 \n",
      "epoch: 314, train_loss: 0.13300503606521227 \n",
      "epoch: 315, train_loss: 0.13351871379483024 \n",
      "epoch: 316, train_loss: 0.13322180359090824 \n",
      "epoch: 317, train_loss: 0.13337245894446378 \n",
      "epoch: 318, train_loss: 0.1329595890166655 \n",
      "epoch: 319, train_loss: 0.13344636135919194 \n",
      "epoch: 320, train_loss: 0.13316500426854916 \n",
      "epoch: 321, train_loss: 0.13336140503862848 \n",
      "epoch: 322, train_loss: 0.1329655298101096 \n",
      "epoch: 323, train_loss: 0.1333737453073475 \n",
      "epoch: 324, train_loss: 0.13299740216785777 \n",
      "epoch: 325, train_loss: 0.13322588187989712 \n",
      "epoch: 326, train_loss: 0.13330140694486722 \n",
      "epoch: 327, train_loss: 0.13302067075456617 \n",
      "epoch: 328, train_loss: 0.13330194186484712 \n",
      "epoch: 329, train_loss: 0.1328071677387994 \n",
      "epoch: 330, train_loss: 0.13314572625944474 \n",
      "epoch: 331, train_loss: 0.13296041458857374 \n",
      "epoch: 332, train_loss: 0.1330648834392079 \n",
      "epoch: 333, train_loss: 0.13317426295668758 \n",
      "epoch: 334, train_loss: 0.133000659041146 \n",
      "epoch: 335, train_loss: 0.13313595679451937 \n",
      "epoch: 336, train_loss: 0.1330887726689765 \n",
      "epoch: 337, train_loss: 0.13311948286501932 \n",
      "epoch: 338, train_loss: 0.1331448837881355 \n",
      "epoch: 339, train_loss: 0.13308129152487713 \n",
      "epoch: 340, train_loss: 0.13284359572352364 \n",
      "epoch: 341, train_loss: 0.13286686958322244 \n",
      "epoch: 342, train_loss: 0.13302814047925365 \n",
      "epoch: 343, train_loss: 0.13301384797001103 \n",
      "epoch: 344, train_loss: 0.13300743075162333 \n",
      "epoch: 345, train_loss: 0.13316097701542343 \n",
      "epoch: 346, train_loss: 0.13286620739947771 \n",
      "epoch: 347, train_loss: 0.1330334236748085 \n",
      "epoch: 348, train_loss: 0.13299992785171505 \n",
      "epoch: 349, train_loss: 0.13281973854344103 \n",
      "epoch: 350, train_loss: 0.13291687511299902 \n",
      "epoch: 351, train_loss: 0.13299507630828467 \n",
      "epoch: 352, train_loss: 0.1330065639500732 \n",
      "epoch: 353, train_loss: 0.13297874948957356 \n",
      "epoch: 354, train_loss: 0.13281626704954166 \n",
      "epoch: 355, train_loss: 0.13286751987191653 \n",
      "epoch: 356, train_loss: 0.13290098579836074 \n",
      "epoch: 357, train_loss: 0.13306576960341296 \n",
      "epoch: 358, train_loss: 0.13280156531409215 \n",
      "epoch: 359, train_loss: 0.13283624678876532 \n",
      "epoch: 360, train_loss: 0.13269641411860103 \n",
      "epoch: 361, train_loss: 0.13289308156589763 \n",
      "epoch: 362, train_loss: 0.13304419587810704 \n",
      "epoch: 363, train_loss: 0.1328500061604382 \n",
      "epoch: 364, train_loss: 0.13295566534342285 \n",
      "epoch: 365, train_loss: 0.132355759373273 \n",
      "epoch: 366, train_loss: 0.1328203533418947 \n",
      "epoch: 367, train_loss: 0.13300646604094124 \n",
      "epoch: 368, train_loss: 0.13236173835298176 \n",
      "epoch: 369, train_loss: 0.1326905576887847 \n",
      "epoch: 370, train_loss: 0.13290274231922608 \n",
      "epoch: 371, train_loss: 0.1326091425126461 \n",
      "epoch: 372, train_loss: 0.1329843740430942 \n",
      "epoch: 373, train_loss: 0.1326598867619347 \n",
      "epoch: 374, train_loss: 0.13246699700654657 \n",
      "epoch: 375, train_loss: 0.1326689762872245 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 376, train_loss: 0.13272511863030317 \n",
      "epoch: 377, train_loss: 0.13285158427081878 \n",
      "epoch: 378, train_loss: 0.1328047196424678 \n",
      "epoch: 379, train_loss: 0.1325399204002449 \n",
      "epoch: 380, train_loss: 0.1325609704516067 \n",
      "epoch: 381, train_loss: 0.13263752542813537 \n",
      "epoch: 382, train_loss: 0.13257375774179472 \n",
      "epoch: 383, train_loss: 0.13292257472370103 \n",
      "epoch: 384, train_loss: 0.13274436064343723 \n",
      "epoch: 385, train_loss: 0.13253600872012342 \n",
      "epoch: 386, train_loss: 0.13260866371151747 \n",
      "epoch: 387, train_loss: 0.13229659174896585 \n",
      "epoch: 388, train_loss: 0.132656609733804 \n",
      "epoch: 389, train_loss: 0.13249571621258313 \n",
      "epoch: 390, train_loss: 0.13284618634025117 \n",
      "epoch: 391, train_loss: 0.1327422353239111 \n",
      "epoch: 392, train_loss: 0.1327197575390664 \n",
      "epoch: 393, train_loss: 0.1326917807724268 \n",
      "epoch: 394, train_loss: 0.13240843521539747 \n",
      "epoch: 395, train_loss: 0.1322086132446417 \n",
      "epoch: 396, train_loss: 0.13204360244281854 \n",
      "epoch: 397, train_loss: 0.1320423927830151 \n",
      "epoch: 398, train_loss: 0.13203330383168643 \n",
      "epoch: 399, train_loss: 0.13203260875424028 \n",
      "epoch: 400, train_loss: 0.13202736531891085 \n",
      "epoch: 401, train_loss: 0.13202296147080264 \n",
      "epoch: 402, train_loss: 0.13201974654303908 \n",
      "epoch: 403, train_loss: 0.13201999461112496 \n",
      "epoch: 404, train_loss: 0.13236356707510047 \n",
      "epoch: 405, train_loss: 0.13247206858070276 \n",
      "epoch: 406, train_loss: 0.13220927144795444 \n",
      "epoch: 407, train_loss: 0.1320312714577599 \n",
      "epoch: 408, train_loss: 0.1325012619040655 \n",
      "epoch: 409, train_loss: 0.13206472201876315 \n",
      "epoch: 410, train_loss: 0.13218280593525866 \n",
      "epoch: 411, train_loss: 0.13233761964899332 \n",
      "epoch: 412, train_loss: 0.13220606233229468 \n",
      "epoch: 413, train_loss: 0.1325399371412838 \n",
      "epoch: 414, train_loss: 0.13226988509117363 \n",
      "epoch: 415, train_loss: 0.13224996255884036 \n",
      "epoch: 416, train_loss: 0.13222435399727345 \n",
      "epoch: 417, train_loss: 0.13200129363935653 \n",
      "epoch: 418, train_loss: 0.13191342280705134 \n",
      "epoch: 419, train_loss: 0.1320441437813726 \n",
      "epoch: 420, train_loss: 0.13217164492198683 \n",
      "epoch: 421, train_loss: 0.13213104290226096 \n",
      "epoch: 422, train_loss: 0.13216008831706336 \n",
      "epoch: 423, train_loss: 0.13209263240787653 \n",
      "epoch: 424, train_loss: 0.1321232814727813 \n",
      "epoch: 425, train_loss: 0.13216310913612847 \n",
      "epoch: 426, train_loss: 0.13216585346736637 \n",
      "epoch: 427, train_loss: 0.13206913970078443 \n",
      "epoch: 428, train_loss: 0.1319006454766341 \n",
      "epoch: 429, train_loss: 0.1320763301018861 \n",
      "epoch: 430, train_loss: 0.13208310549502705 \n",
      "epoch: 431, train_loss: 0.1320855265026647 \n",
      "epoch: 432, train_loss: 0.13198911397423158 \n",
      "epoch: 433, train_loss: 0.1320626562566132 \n",
      "epoch: 434, train_loss: 0.13196836180854254 \n",
      "epoch: 435, train_loss: 0.13204290434370247 \n",
      "epoch: 436, train_loss: 0.13194077498536153 \n",
      "epoch: 437, train_loss: 0.13196618926606993 \n",
      "epoch: 438, train_loss: 0.13201710445127246 \n",
      "epoch: 439, train_loss: 0.13189939402658868 \n",
      "epoch: 440, train_loss: 0.13198933512962013 \n",
      "epoch: 441, train_loss: 0.13189592825552462 \n",
      "epoch: 442, train_loss: 0.13197917744411264 \n",
      "epoch: 443, train_loss: 0.13196718191779971 \n",
      "epoch: 444, train_loss: 0.1322905917225047 \n",
      "epoch: 445, train_loss: 0.1319686758224268 \n",
      "epoch: 446, train_loss: 0.13185296826671988 \n",
      "epoch: 447, train_loss: 0.13193297132706439 \n",
      "epoch: 448, train_loss: 0.13183822849618543 \n",
      "epoch: 449, train_loss: 0.13208816100291051 \n",
      "epoch: 450, train_loss: 0.13190514637647338 \n",
      "epoch: 451, train_loss: 0.13190383707622721 \n",
      "epoch: 452, train_loss: 0.1320874293789073 \n",
      "epoch: 453, train_loss: 0.13182713780582012 \n",
      "epoch: 454, train_loss: 0.13187300503459154 \n",
      "epoch: 455, train_loss: 0.13178402023433225 \n",
      "epoch: 456, train_loss: 0.13212311833245716 \n",
      "epoch: 457, train_loss: 0.1316947200612304 \n",
      "epoch: 458, train_loss: 0.1317526716536777 \n",
      "epoch: 459, train_loss: 0.13206099621185552 \n",
      "epoch: 460, train_loss: 0.13177018831877263 \n",
      "epoch: 461, train_loss: 0.13167705964677026 \n",
      "epoch: 462, train_loss: 0.13176210547642156 \n",
      "epoch: 463, train_loss: 0.13177561767354656 \n",
      "epoch: 464, train_loss: 0.13173829668074405 \n",
      "epoch: 465, train_loss: 0.13173725743081427 \n",
      "epoch: 466, train_loss: 0.13174168809745695 \n",
      "epoch: 467, train_loss: 0.1317293490477267 \n",
      "epoch: 468, train_loss: 0.1317240092803964 \n",
      "epoch: 469, train_loss: 0.1317183598516111 \n",
      "epoch: 470, train_loss: 0.13171086162993154 \n",
      "epoch: 471, train_loss: 0.13170514428206273 \n",
      "epoch: 472, train_loss: 0.13169947748150349 \n",
      "epoch: 473, train_loss: 0.13169486912087433 \n",
      "epoch: 474, train_loss: 0.13168819903233825 \n",
      "epoch: 475, train_loss: 0.13168771561218978 \n",
      "epoch: 476, train_loss: 0.1316733025988978 \n",
      "epoch: 477, train_loss: 0.13192934235740592 \n",
      "epoch: 478, train_loss: 0.13164487504909836 \n",
      "epoch: 479, train_loss: 0.13166830629153034 \n",
      "epoch: 480, train_loss: 0.13187683895993635 \n",
      "epoch: 481, train_loss: 0.13182023570142792 \n",
      "epoch: 482, train_loss: 0.13182620885872304 \n",
      "epoch: 483, train_loss: 0.131590312457238 \n",
      "epoch: 484, train_loss: 0.1316174262616498 \n",
      "epoch: 485, train_loss: 0.13162494514653061 \n",
      "epoch: 486, train_loss: 0.13162547928821733 \n",
      "epoch: 487, train_loss: 0.1316388635800205 \n",
      "epoch: 488, train_loss: 0.1316032409717638 \n",
      "epoch: 489, train_loss: 0.1316525173800518 \n",
      "epoch: 490, train_loss: 0.13157747894881794 \n",
      "epoch: 491, train_loss: 0.13176072362502284 \n",
      "epoch: 492, train_loss: 0.13161125956190065 \n",
      "epoch: 493, train_loss: 0.13181156201450356 \n",
      "epoch: 494, train_loss: 0.13158054891552784 \n",
      "epoch: 495, train_loss: 0.131461879706924 \n",
      "epoch: 496, train_loss: 0.13168186932736495 \n",
      "epoch: 497, train_loss: 0.13155427868249792 \n",
      "epoch: 498, train_loss: 0.13141667554904796 \n",
      "epoch: 499, train_loss: 0.13183242459578523 \n",
      "epoch: 500, train_loss: 0.13186107858794754 \n",
      "epoch: 501, train_loss: 0.13153152670670318 \n",
      "epoch: 502, train_loss: 0.1313751775704595 \n",
      "epoch: 503, train_loss: 0.1317635428076857 \n",
      "epoch: 504, train_loss: 0.13180104280304145 \n",
      "epoch: 505, train_loss: 0.13130594504810103 \n",
      "epoch: 506, train_loss: 0.13170682250282634 \n",
      "epoch: 507, train_loss: 0.13166088975320975 \n",
      "epoch: 508, train_loss: 0.13134857368164202 \n",
      "epoch: 509, train_loss: 0.13170977874365503 \n",
      "epoch: 510, train_loss: 0.1317882737285942 \n",
      "epoch: 511, train_loss: 0.13125987236002665 \n",
      "epoch: 512, train_loss: 0.13169566176087769 \n",
      "epoch: 513, train_loss: 0.1312381076636383 \n",
      "epoch: 514, train_loss: 0.1316828107441084 \n",
      "epoch: 515, train_loss: 0.13167270996393599 \n",
      "epoch: 516, train_loss: 0.13123749679644475 \n",
      "epoch: 517, train_loss: 0.13164894808368832 \n",
      "epoch: 518, train_loss: 0.13164013197087931 \n",
      "epoch: 519, train_loss: 0.13160891403823166 \n",
      "epoch: 520, train_loss: 0.13113239231589785 \n",
      "epoch: 521, train_loss: 0.13160103301104584 \n",
      "epoch: 522, train_loss: 0.13153564760058645 \n",
      "epoch: 523, train_loss: 0.13111261869449312 \n",
      "epoch: 524, train_loss: 0.13108456379598554 \n",
      "epoch: 525, train_loss: 0.1315652387342979 \n",
      "epoch: 526, train_loss: 0.13155259406573971 \n",
      "epoch: 527, train_loss: 0.13153388004352373 \n",
      "epoch: 528, train_loss: 0.13150126478879479 \n",
      "epoch: 529, train_loss: 0.1310516912883169 \n",
      "epoch: 530, train_loss: 0.13152723440682368 \n",
      "epoch: 531, train_loss: 0.1315321602296952 \n",
      "epoch: 532, train_loss: 0.13114822607886348 \n",
      "epoch: 533, train_loss: 0.13154825291755654 \n",
      "epoch: 534, train_loss: 0.13149523984522302 \n",
      "epoch: 535, train_loss: 0.13127490501298922 \n",
      "epoch: 536, train_loss: 0.13102299186482355 \n",
      "epoch: 537, train_loss: 0.1314841838466831 \n",
      "epoch: 538, train_loss: 0.13119377018493317 \n",
      "epoch: 539, train_loss: 0.13100581316008914 \n",
      "epoch: 540, train_loss: 0.13097095080138543 \n",
      "epoch: 541, train_loss: 0.13144405903419404 \n",
      "epoch: 542, train_loss: 0.13142638136146043 \n",
      "epoch: 543, train_loss: 0.13130994527693893 \n",
      "epoch: 544, train_loss: 0.1309704754872848 \n",
      "epoch: 545, train_loss: 0.13146054529427684 \n",
      "epoch: 546, train_loss: 0.13143956634104037 \n",
      "epoch: 547, train_loss: 0.13140776921710137 \n",
      "epoch: 548, train_loss: 0.1312251649832065 \n",
      "epoch: 549, train_loss: 0.1308888328403569 \n",
      "epoch: 550, train_loss: 0.13136045327375703 \n",
      "epoch: 551, train_loss: 0.1310386671572595 \n",
      "epoch: 552, train_loss: 0.13091188420446653 \n",
      "epoch: 553, train_loss: 0.1308551617389545 \n",
      "epoch: 554, train_loss: 0.13107211378948813 \n",
      "epoch: 555, train_loss: 0.13136947519459374 \n",
      "epoch: 556, train_loss: 0.13121991084966925 \n",
      "epoch: 557, train_loss: 0.13085800756210847 \n",
      "epoch: 558, train_loss: 0.1308264484062566 \n",
      "epoch: 559, train_loss: 0.13127727403121264 \n",
      "epoch: 560, train_loss: 0.13086029399795873 \n",
      "epoch: 561, train_loss: 0.13131193711974373 \n",
      "epoch: 562, train_loss: 0.13083869440864312 \n",
      "epoch: 563, train_loss: 0.1312801687281645 \n",
      "epoch: 564, train_loss: 0.13080747924917216 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 565, train_loss: 0.13102940631885382 \n",
      "epoch: 566, train_loss: 0.1308115683014077 \n",
      "epoch: 567, train_loss: 0.1308086369776201 \n",
      "epoch: 568, train_loss: 0.13121941434961076 \n",
      "epoch: 569, train_loss: 0.13076319539168552 \n",
      "epoch: 570, train_loss: 0.13097818677327927 \n",
      "epoch: 571, train_loss: 0.13080743875497997 \n",
      "epoch: 572, train_loss: 0.13108614920043393 \n",
      "epoch: 573, train_loss: 0.1307425048705275 \n",
      "epoch: 574, train_loss: 0.13101851961088848 \n",
      "epoch: 575, train_loss: 0.1307321450073335 \n",
      "epoch: 576, train_loss: 0.1310117699091771 \n",
      "epoch: 577, train_loss: 0.13072981916208254 \n",
      "epoch: 578, train_loss: 0.1309908675679561 \n",
      "epoch: 579, train_loss: 0.13073087918620221 \n",
      "epoch: 580, train_loss: 0.13101405628846977 \n",
      "epoch: 581, train_loss: 0.13089729281889725 \n",
      "epoch: 582, train_loss: 0.13070604906326475 \n",
      "epoch: 583, train_loss: 0.1308813152513532 \n",
      "epoch: 584, train_loss: 0.13100228891359028 \n",
      "epoch: 585, train_loss: 0.13085580921642703 \n",
      "epoch: 586, train_loss: 0.13070881909408266 \n",
      "epoch: 587, train_loss: 0.1309507037953913 \n",
      "epoch: 588, train_loss: 0.1306789289671205 \n",
      "epoch: 589, train_loss: 0.13098592896961317 \n",
      "epoch: 590, train_loss: 0.13068328387954842 \n",
      "epoch: 591, train_loss: 0.13096277058695743 \n",
      "epoch: 592, train_loss: 0.1307554871508716 \n",
      "epoch: 593, train_loss: 0.13069870059243585 \n",
      "epoch: 594, train_loss: 0.13094156060173265 \n",
      "epoch: 595, train_loss: 0.13087343452197855 \n",
      "epoch: 596, train_loss: 0.13084662112187806 \n",
      "epoch: 597, train_loss: 0.13090840870320183 \n",
      "epoch: 598, train_loss: 0.13065752021656182 \n",
      "epoch: 599, train_loss: 0.13094596702754133 \n",
      "epoch: 600, train_loss: 0.1306090780343422 \n",
      "epoch: 601, train_loss: 0.1308891133718552 \n",
      "epoch: 602, train_loss: 0.13062733782381267 \n",
      "epoch: 603, train_loss: 0.13091000884311035 \n",
      "epoch: 604, train_loss: 0.13063167825840982 \n",
      "epoch: 605, train_loss: 0.13083252995322003 \n",
      "epoch: 606, train_loss: 0.13055670982236 \n",
      "epoch: 607, train_loss: 0.13081268599489493 \n",
      "epoch: 608, train_loss: 0.1307732168349444 \n",
      "epoch: 609, train_loss: 0.13077485406007192 \n",
      "epoch: 610, train_loss: 0.13075191335799582 \n",
      "epoch: 611, train_loss: 0.13071643567759458 \n",
      "epoch: 612, train_loss: 0.1306483985014773 \n",
      "epoch: 613, train_loss: 0.13067736898695445 \n",
      "epoch: 614, train_loss: 0.13066274308243547 \n",
      "epoch: 615, train_loss: 0.1306397229914923 \n",
      "epoch: 616, train_loss: 0.13060133632563645 \n",
      "epoch: 617, train_loss: 0.1306174242319925 \n",
      "epoch: 618, train_loss: 0.13059020412591138 \n",
      "epoch: 619, train_loss: 0.13058988113177436 \n",
      "epoch: 620, train_loss: 0.13057437596440835 \n",
      "epoch: 621, train_loss: 0.13056732334610313 \n",
      "epoch: 622, train_loss: 0.13056064612279872 \n",
      "epoch: 623, train_loss: 0.13055020241655854 \n",
      "epoch: 624, train_loss: 0.13053834548336232 \n",
      "epoch: 625, train_loss: 0.13052090167492514 \n",
      "epoch: 626, train_loss: 0.13052461422341308 \n",
      "epoch: 627, train_loss: 0.130513848311403 \n",
      "epoch: 628, train_loss: 0.13050198159459228 \n",
      "epoch: 629, train_loss: 0.13049628225109225 \n",
      "epoch: 630, train_loss: 0.13048460334986342 \n",
      "epoch: 631, train_loss: 0.1304765604709447 \n",
      "epoch: 632, train_loss: 0.13046983626716185 \n",
      "epoch: 633, train_loss: 0.13046428283756892 \n",
      "epoch: 634, train_loss: 0.1304597869260625 \n",
      "epoch: 635, train_loss: 0.1304532188956942 \n",
      "epoch: 636, train_loss: 0.13044360404765007 \n",
      "epoch: 637, train_loss: 0.13043736813942625 \n",
      "epoch: 638, train_loss: 0.13042869448376734 \n",
      "epoch: 639, train_loss: 0.13042244437915354 \n",
      "epoch: 640, train_loss: 0.13041506996725477 \n",
      "epoch: 641, train_loss: 0.13040755367700194 \n",
      "epoch: 642, train_loss: 0.13040008070829429 \n",
      "epoch: 643, train_loss: 0.1303912427787574 \n",
      "epoch: 644, train_loss: 0.1304014654570872 \n",
      "epoch: 645, train_loss: 0.13037784279056286 \n",
      "epoch: 646, train_loss: 0.13037135909783304 \n",
      "epoch: 647, train_loss: 0.13036208596819343 \n",
      "epoch: 648, train_loss: 0.1303721384784535 \n",
      "epoch: 649, train_loss: 0.13034795579467148 \n",
      "epoch: 650, train_loss: 0.13033771471102745 \n",
      "epoch: 651, train_loss: 0.13033260301335886 \n",
      "epoch: 652, train_loss: 0.13032744632853058 \n",
      "epoch: 653, train_loss: 0.1303147097238828 \n",
      "epoch: 654, train_loss: 0.1303097186760857 \n",
      "epoch: 655, train_loss: 0.13030556072532865 \n",
      "epoch: 656, train_loss: 0.1302927636648579 \n",
      "epoch: 657, train_loss: 0.13028006537665773 \n",
      "epoch: 658, train_loss: 0.13027452806991513 \n",
      "epoch: 659, train_loss: 0.1302663621180241 \n",
      "epoch: 660, train_loss: 0.13026039375060475 \n",
      "epoch: 661, train_loss: 0.13025793894184715 \n",
      "epoch: 662, train_loss: 0.13024870592994608 \n",
      "epoch: 663, train_loss: 0.1302524828789201 \n",
      "epoch: 664, train_loss: 0.13024005071849193 \n",
      "epoch: 665, train_loss: 0.13023855321700167 \n",
      "epoch: 666, train_loss: 0.13023161117777088 \n",
      "epoch: 667, train_loss: 0.13024068778008768 \n",
      "epoch: 668, train_loss: 0.13021389433187588 \n",
      "epoch: 669, train_loss: 0.1302133108962712 \n",
      "epoch: 670, train_loss: 0.13020619192545255 \n",
      "epoch: 671, train_loss: 0.1301996750777545 \n",
      "epoch: 672, train_loss: 0.13019498576234811 \n",
      "epoch: 673, train_loss: 0.13041053694158772 \n",
      "epoch: 674, train_loss: 0.13018597896938985 \n",
      "epoch: 675, train_loss: 0.13032097292630487 \n",
      "epoch: 676, train_loss: 0.13038540730634696 \n",
      "epoch: 677, train_loss: 0.1301867494491672 \n",
      "epoch: 678, train_loss: 0.13037600574064478 \n",
      "epoch: 679, train_loss: 0.13037444695401634 \n",
      "epoch: 680, train_loss: 0.13015675783202138 \n",
      "epoch: 681, train_loss: 0.1303947198174685 \n",
      "epoch: 682, train_loss: 0.13015731517051063 \n",
      "epoch: 683, train_loss: 0.1303602569226866 \n",
      "epoch: 684, train_loss: 0.13031751687439536 \n",
      "epoch: 685, train_loss: 0.13033701769820252 \n",
      "epoch: 686, train_loss: 0.13010792851700265 \n",
      "epoch: 687, train_loss: 0.13006976520420482 \n",
      "epoch: 688, train_loss: 0.12998453436578827 \n",
      "epoch: 689, train_loss: 0.13032185248679365 \n",
      "epoch: 690, train_loss: 0.13007594696489233 \n",
      "epoch: 691, train_loss: 0.12994142346113596 \n",
      "epoch: 692, train_loss: 0.130115617607593 \n",
      "epoch: 693, train_loss: 0.12997289947857485 \n",
      "epoch: 694, train_loss: 0.13025655382894585 \n",
      "epoch: 695, train_loss: 0.12991972316552453 \n",
      "epoch: 696, train_loss: 0.12996214715758206 \n",
      "epoch: 697, train_loss: 0.12998515517330486 \n",
      "epoch: 698, train_loss: 0.13000720624901022 \n",
      "epoch: 699, train_loss: 0.1302118072047048 \n",
      "epoch: 700, train_loss: 0.13000472111464964 \n",
      "epoch: 701, train_loss: 0.1299333341982461 \n",
      "epoch: 702, train_loss: 0.1299988881789562 \n",
      "epoch: 703, train_loss: 0.12994206363584618 \n",
      "epoch: 704, train_loss: 0.12993163352767642 \n",
      "epoch: 705, train_loss: 0.12992118945697628 \n",
      "epoch: 706, train_loss: 0.12989255100015473 \n",
      "epoch: 707, train_loss: 0.12989752251050643 \n",
      "epoch: 708, train_loss: 0.1298511292699632 \n",
      "epoch: 709, train_loss: 0.13015102613426588 \n",
      "epoch: 710, train_loss: 0.12993197369592377 \n",
      "epoch: 711, train_loss: 0.12983084795162164 \n",
      "epoch: 712, train_loss: 0.13011342237260395 \n",
      "epoch: 713, train_loss: 0.12998758808586375 \n",
      "epoch: 714, train_loss: 0.1300176088234116 \n",
      "epoch: 715, train_loss: 0.13001604510428652 \n",
      "epoch: 716, train_loss: 0.12977460184866904 \n",
      "epoch: 717, train_loss: 0.13007139463174974 \n",
      "epoch: 718, train_loss: 0.12987460598315398 \n",
      "epoch: 719, train_loss: 0.12973035698128785 \n",
      "epoch: 720, train_loss: 0.12977014888899013 \n",
      "epoch: 721, train_loss: 0.12973187866589297 \n",
      "epoch: 722, train_loss: 0.1300130733373242 \n",
      "epoch: 723, train_loss: 0.12981318166402525 \n",
      "epoch: 724, train_loss: 0.12969262078068683 \n",
      "epoch: 725, train_loss: 0.1300304694229851 \n",
      "epoch: 726, train_loss: 0.12990957766283415 \n",
      "epoch: 727, train_loss: 0.13002470803729782 \n",
      "epoch: 728, train_loss: 0.12988943009205278 \n",
      "epoch: 729, train_loss: 0.12970791884684654 \n",
      "epoch: 730, train_loss: 0.12966997073198622 \n",
      "epoch: 731, train_loss: 0.12967476835664646 \n",
      "epoch: 732, train_loss: 0.12966203805531046 \n",
      "epoch: 733, train_loss: 0.12966306862515 \n",
      "epoch: 734, train_loss: 0.12966991037408243 \n",
      "epoch: 735, train_loss: 0.12967035307010255 \n",
      "epoch: 736, train_loss: 0.1296536134779861 \n",
      "epoch: 737, train_loss: 0.12964810681432518 \n",
      "epoch: 738, train_loss: 0.1296435127692781 \n",
      "epoch: 739, train_loss: 0.1296381469360619 \n",
      "epoch: 740, train_loss: 0.12963329103320884 \n",
      "epoch: 741, train_loss: 0.12962701420707257 \n",
      "epoch: 742, train_loss: 0.12962241473450478 \n",
      "epoch: 743, train_loss: 0.12961499134539325 \n",
      "epoch: 744, train_loss: 0.12961145443443867 \n",
      "epoch: 745, train_loss: 0.12960328900329318 \n",
      "epoch: 746, train_loss: 0.12959808841399392 \n",
      "epoch: 747, train_loss: 0.12959027367014084 \n",
      "epoch: 748, train_loss: 0.12958311432690922 \n",
      "epoch: 749, train_loss: 0.12957652654712168 \n",
      "epoch: 750, train_loss: 0.1295690689152716 \n",
      "epoch: 751, train_loss: 0.1295629186758964 \n",
      "epoch: 752, train_loss: 0.12955562969298964 \n",
      "epoch: 753, train_loss: 0.12954970049501024 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 754, train_loss: 0.12954379298863428 \n",
      "epoch: 755, train_loss: 0.12953893891097862 \n",
      "epoch: 756, train_loss: 0.129535939049711 \n",
      "epoch: 757, train_loss: 0.12954071657268657 \n",
      "epoch: 758, train_loss: 0.12967488918213188 \n",
      "epoch: 759, train_loss: 0.12961447213099767 \n",
      "epoch: 760, train_loss: 0.12954531571542452 \n",
      "epoch: 761, train_loss: 0.12951852066836012 \n",
      "epoch: 762, train_loss: 0.1296649310232112 \n",
      "epoch: 763, train_loss: 0.12959831756925339 \n",
      "epoch: 764, train_loss: 0.1297496874219966 \n",
      "epoch: 765, train_loss: 0.1295920492664549 \n",
      "epoch: 766, train_loss: 0.12949083730045638 \n",
      "epoch: 767, train_loss: 0.1295412153091245 \n",
      "epoch: 768, train_loss: 0.12948901530632592 \n",
      "epoch: 769, train_loss: 0.12956548771368556 \n",
      "epoch: 770, train_loss: 0.129478271771593 \n",
      "epoch: 771, train_loss: 0.12950703838849817 \n",
      "epoch: 772, train_loss: 0.12933376060639462 \n",
      "epoch: 773, train_loss: 0.12945669406948726 \n",
      "epoch: 774, train_loss: 0.12962267869872365 \n",
      "epoch: 775, train_loss: 0.1296117754559859 \n",
      "epoch: 776, train_loss: 0.12960485920311796 \n",
      "epoch: 777, train_loss: 0.12959809476205453 \n",
      "epoch: 778, train_loss: 0.12959275980506949 \n",
      "epoch: 779, train_loss: 0.12958768003664498 \n",
      "epoch: 780, train_loss: 0.12957962919286398 \n",
      "epoch: 781, train_loss: 0.12957071518581012 \n",
      "epoch: 782, train_loss: 0.12956347857981837 \n",
      "epoch: 783, train_loss: 0.12955574126818023 \n",
      "epoch: 784, train_loss: 0.1295485671817029 \n",
      "epoch: 785, train_loss: 0.1295407995671228 \n",
      "epoch: 786, train_loss: 0.12953371640357234 \n",
      "epoch: 787, train_loss: 0.1295256199381577 \n",
      "epoch: 788, train_loss: 0.1295188881877789 \n",
      "epoch: 789, train_loss: 0.1295097816735652 \n",
      "epoch: 790, train_loss: 0.1295045007632047 \n",
      "epoch: 791, train_loss: 0.1294917136266093 \n",
      "epoch: 792, train_loss: 0.12949207232202128 \n",
      "epoch: 793, train_loss: 0.12946173500198332 \n",
      "epoch: 794, train_loss: 0.12946610345282814 \n",
      "epoch: 795, train_loss: 0.1294633107929158 \n",
      "epoch: 796, train_loss: 0.12945610923407805 \n",
      "epoch: 797, train_loss: 0.12944513033478203 \n",
      "epoch: 798, train_loss: 0.12944325866132303 \n",
      "epoch: 799, train_loss: 0.12941492715928493 \n",
      "epoch: 800, train_loss: 0.12941647328148298 \n",
      "epoch: 801, train_loss: 0.12941405871266953 \n",
      "epoch: 802, train_loss: 0.1294079073789532 \n",
      "epoch: 803, train_loss: 0.12939383506632915 \n",
      "epoch: 804, train_loss: 0.1293974800576106 \n",
      "epoch: 805, train_loss: 0.12934651561176036 \n",
      "epoch: 806, train_loss: 0.12929330708750292 \n",
      "epoch: 807, train_loss: 0.12927739754500361 \n",
      "epoch: 808, train_loss: 0.1292699339742906 \n",
      "epoch: 809, train_loss: 0.12918189596605434 \n",
      "epoch: 810, train_loss: 0.1291518580821625 \n",
      "epoch: 811, train_loss: 0.129162656322303 \n",
      "epoch: 812, train_loss: 0.12916019006855126 \n",
      "epoch: 813, train_loss: 0.12915048455452818 \n",
      "epoch: 814, train_loss: 0.12916433301522384 \n",
      "epoch: 815, train_loss: 0.12924983191176595 \n",
      "epoch: 816, train_loss: 0.12922100646989595 \n",
      "epoch: 817, train_loss: 0.12920168543827917 \n",
      "epoch: 818, train_loss: 0.12919813593472662 \n",
      "epoch: 819, train_loss: 0.12916211977227346 \n",
      "epoch: 820, train_loss: 0.12918423434199042 \n",
      "epoch: 821, train_loss: 0.12919026250226232 \n",
      "epoch: 822, train_loss: 0.1291677267456169 \n",
      "epoch: 823, train_loss: 0.12915951365050105 \n",
      "epoch: 824, train_loss: 0.12909014056575863 \n",
      "epoch: 825, train_loss: 0.12907074232757995 \n",
      "epoch: 826, train_loss: 0.12911610751961716 \n",
      "epoch: 827, train_loss: 0.12916985771086364 \n",
      "epoch: 828, train_loss: 0.129110088363677 \n",
      "epoch: 829, train_loss: 0.12910423408503735 \n",
      "epoch: 830, train_loss: 0.12902849839582348 \n",
      "epoch: 831, train_loss: 0.12904412251832403 \n",
      "epoch: 832, train_loss: 0.12909117413171523 \n",
      "epoch: 833, train_loss: 0.1290820242884747 \n",
      "epoch: 834, train_loss: 0.12906520320810883 \n",
      "epoch: 835, train_loss: 0.1289885499705462 \n",
      "epoch: 836, train_loss: 0.1289788991887887 \n",
      "epoch: 837, train_loss: 0.12897769395050274 \n",
      "epoch: 838, train_loss: 0.12897666309466982 \n",
      "epoch: 839, train_loss: 0.12905479314161855 \n",
      "epoch: 840, train_loss: 0.12907628039246516 \n",
      "epoch: 841, train_loss: 0.12900915646820318 \n",
      "epoch: 842, train_loss: 0.12900779455067182 \n",
      "epoch: 843, train_loss: 0.12896252113489384 \n",
      "epoch: 844, train_loss: 0.12899236904695233 \n",
      "epoch: 845, train_loss: 0.12897851430502452 \n",
      "epoch: 846, train_loss: 0.12893039300098036 \n",
      "epoch: 847, train_loss: 0.12892929063268477 \n",
      "epoch: 848, train_loss: 0.12889853086839761 \n",
      "epoch: 849, train_loss: 0.12887606099181295 \n",
      "epoch: 850, train_loss: 0.12886511131920145 \n",
      "epoch: 851, train_loss: 0.12885802041975125 \n",
      "epoch: 852, train_loss: 0.1286417117915511 \n",
      "epoch: 853, train_loss: 0.1288830954510209 \n",
      "epoch: 854, train_loss: 0.1289530336972157 \n",
      "epoch: 855, train_loss: 0.1289231853353963 \n",
      "epoch: 856, train_loss: 0.12891483518529834 \n",
      "epoch: 857, train_loss: 0.12888698480973285 \n",
      "epoch: 858, train_loss: 0.12888285406374478 \n",
      "epoch: 859, train_loss: 0.1288364227187861 \n",
      "epoch: 860, train_loss: 0.1288601273667327 \n",
      "epoch: 861, train_loss: 0.12885407972007218 \n",
      "epoch: 862, train_loss: 0.12883800536721574 \n",
      "epoch: 863, train_loss: 0.12883800118752964 \n",
      "epoch: 864, train_loss: 0.12878074728086042 \n",
      "epoch: 865, train_loss: 0.12877507491973533 \n",
      "epoch: 866, train_loss: 0.1287972835809633 \n",
      "epoch: 867, train_loss: 0.1288126707428854 \n",
      "epoch: 868, train_loss: 0.12879530455558402 \n",
      "epoch: 869, train_loss: 0.1288027744596144 \n",
      "epoch: 870, train_loss: 0.1287694159369213 \n",
      "epoch: 871, train_loss: 0.1287756289296975 \n",
      "epoch: 872, train_loss: 0.12873139101091244 \n",
      "epoch: 873, train_loss: 0.12875538658341165 \n",
      "epoch: 874, train_loss: 0.12875078852251468 \n",
      "epoch: 875, train_loss: 0.12871110456603474 \n",
      "epoch: 876, train_loss: 0.12874477330116055 \n",
      "epoch: 877, train_loss: 0.12874866484143777 \n",
      "epoch: 878, train_loss: 0.12871283741608155 \n",
      "epoch: 879, train_loss: 0.12871358664763316 \n",
      "epoch: 880, train_loss: 0.12865924630611977 \n",
      "epoch: 881, train_loss: 0.1286448587221183 \n",
      "epoch: 882, train_loss: 0.12861200450037516 \n",
      "epoch: 883, train_loss: 0.12821869002820896 \n",
      "epoch: 884, train_loss: 0.12843792938769658 \n",
      "epoch: 885, train_loss: 0.12858585841867334 \n",
      "epoch: 886, train_loss: 0.12819554007838727 \n",
      "epoch: 887, train_loss: 0.12842560093639868 \n",
      "epoch: 888, train_loss: 0.12857077264424835 \n",
      "epoch: 889, train_loss: 0.12848964970400945 \n",
      "epoch: 890, train_loss: 0.1285722177879247 \n",
      "epoch: 891, train_loss: 0.12855103701761394 \n",
      "epoch: 892, train_loss: 0.1284499574461865 \n",
      "epoch: 893, train_loss: 0.128632168072972 \n",
      "epoch: 894, train_loss: 0.1283227781385741 \n",
      "epoch: 895, train_loss: 0.12841944585757242 \n",
      "epoch: 896, train_loss: 0.12851496308784363 \n",
      "epoch: 897, train_loss: 0.12859124614508932 \n",
      "epoch: 898, train_loss: 0.12863756924539838 \n",
      "epoch: 899, train_loss: 0.1284338092141571 \n",
      "epoch: 900, train_loss: 0.128602656296925 \n",
      "epoch: 901, train_loss: 0.12860506266493932 \n",
      "epoch: 902, train_loss: 0.12850230926428227 \n",
      "epoch: 903, train_loss: 0.1285905762073495 \n",
      "epoch: 904, train_loss: 0.12858687862432677 \n",
      "epoch: 905, train_loss: 0.1285123257740559 \n",
      "epoch: 906, train_loss: 0.12857248038234073 \n",
      "epoch: 907, train_loss: 0.12855704981066973 \n",
      "epoch: 908, train_loss: 0.12845810220779494 \n",
      "epoch: 909, train_loss: 0.12848843001460353 \n",
      "epoch: 910, train_loss: 0.12852578571757425 \n",
      "epoch: 911, train_loss: 0.12818315782210837 \n",
      "epoch: 912, train_loss: 0.12832580851489012 \n",
      "epoch: 913, train_loss: 0.12847208468434537 \n",
      "epoch: 914, train_loss: 0.12845542707848467 \n",
      "epoch: 915, train_loss: 0.12826383270044034 \n",
      "epoch: 916, train_loss: 0.1284048467126259 \n",
      "epoch: 917, train_loss: 0.12844814251481612 \n",
      "epoch: 918, train_loss: 0.12835940379845517 \n",
      "epoch: 919, train_loss: 0.12832397127113004 \n",
      "epoch: 920, train_loss: 0.12833279548085394 \n",
      "epoch: 921, train_loss: 0.12833293961767936 \n",
      "epoch: 922, train_loss: 0.12836876799762195 \n",
      "epoch: 923, train_loss: 0.12841221798629826 \n",
      "epoch: 924, train_loss: 0.1282051134871449 \n",
      "epoch: 925, train_loss: 0.1283482963849355 \n",
      "epoch: 926, train_loss: 0.12834217151427527 \n",
      "epoch: 927, train_loss: 0.12834448483261132 \n",
      "epoch: 928, train_loss: 0.12804860343030816 \n",
      "epoch: 929, train_loss: 0.12827978964699102 \n",
      "epoch: 930, train_loss: 0.12830958667813525 \n",
      "epoch: 931, train_loss: 0.1283073635888338 \n",
      "epoch: 932, train_loss: 0.1283131810569663 \n",
      "epoch: 933, train_loss: 0.12829139966691672 \n",
      "epoch: 934, train_loss: 0.12829872006510593 \n",
      "epoch: 935, train_loss: 0.1282849283306165 \n",
      "epoch: 936, train_loss: 0.1282548758443288 \n",
      "epoch: 937, train_loss: 0.12822949706856504 \n",
      "epoch: 938, train_loss: 0.12822093917448005 \n",
      "epoch: 939, train_loss: 0.12821362586125154 \n",
      "epoch: 940, train_loss: 0.12820207005127698 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 941, train_loss: 0.12820098963611307 \n",
      "epoch: 942, train_loss: 0.12819883912196425 \n",
      "epoch: 943, train_loss: 0.12817436140705876 \n",
      "epoch: 944, train_loss: 0.12817624651247098 \n",
      "epoch: 945, train_loss: 0.1281750587674152 \n",
      "epoch: 946, train_loss: 0.12815065645326243 \n",
      "epoch: 947, train_loss: 0.12815005932809223 \n",
      "epoch: 948, train_loss: 0.12814774190236128 \n",
      "epoch: 949, train_loss: 0.128128728064391 \n",
      "epoch: 950, train_loss: 0.12812226387981143 \n",
      "epoch: 951, train_loss: 0.12811280784721382 \n",
      "epoch: 952, train_loss: 0.12810401626841555 \n",
      "epoch: 953, train_loss: 0.12809353599816106 \n",
      "epoch: 954, train_loss: 0.12808673559109118 \n",
      "epoch: 955, train_loss: 0.12807759855562115 \n",
      "epoch: 956, train_loss: 0.12806852143315126 \n",
      "epoch: 957, train_loss: 0.12805941304286161 \n",
      "epoch: 958, train_loss: 0.12805103032851556 \n",
      "epoch: 959, train_loss: 0.12804268622290257 \n",
      "epoch: 960, train_loss: 0.12803439927459176 \n",
      "epoch: 961, train_loss: 0.12802645937571527 \n",
      "epoch: 962, train_loss: 0.1280189097275746 \n",
      "epoch: 963, train_loss: 0.1280121895581676 \n",
      "epoch: 964, train_loss: 0.12800767443497377 \n",
      "epoch: 965, train_loss: 0.12800835757257345 \n",
      "epoch: 966, train_loss: 0.128008554735131 \n",
      "epoch: 967, train_loss: 0.12800528772353148 \n",
      "epoch: 968, train_loss: 0.12799748870192107 \n",
      "epoch: 969, train_loss: 0.12799032100785557 \n",
      "epoch: 970, train_loss: 0.12798258152956815 \n",
      "epoch: 971, train_loss: 0.12797465482652642 \n",
      "epoch: 972, train_loss: 0.12796653114121936 \n",
      "epoch: 973, train_loss: 0.12795843551240652 \n",
      "epoch: 974, train_loss: 0.12795010443797594 \n",
      "epoch: 975, train_loss: 0.12794205881215837 \n",
      "epoch: 976, train_loss: 0.1279333332379123 \n",
      "epoch: 977, train_loss: 0.12792570210280732 \n",
      "epoch: 978, train_loss: 0.1279158870440176 \n",
      "epoch: 979, train_loss: 0.1279097532049533 \n",
      "epoch: 980, train_loss: 0.12789721007868848 \n",
      "epoch: 981, train_loss: 0.12789427381637744 \n",
      "epoch: 982, train_loss: 0.12787901396733548 \n",
      "epoch: 983, train_loss: 0.1278771125824989 \n",
      "epoch: 984, train_loss: 0.12786155987241057 \n",
      "epoch: 985, train_loss: 0.12785916671148242 \n",
      "epoch: 986, train_loss: 0.1278427013357213 \n",
      "epoch: 987, train_loss: 0.12784051835822807 \n",
      "epoch: 988, train_loss: 0.127823599172041 \n",
      "epoch: 989, train_loss: 0.12782110746106295 \n",
      "epoch: 990, train_loss: 0.12780424505134022 \n",
      "epoch: 991, train_loss: 0.1278009722984493 \n",
      "epoch: 992, train_loss: 0.12778481478395984 \n",
      "epoch: 993, train_loss: 0.12778037934924233 \n",
      "epoch: 994, train_loss: 0.12776534556571287 \n",
      "epoch: 995, train_loss: 0.12775941514610237 \n",
      "epoch: 996, train_loss: 0.12774569362141433 \n",
      "epoch: 997, train_loss: 0.12773801717527164 \n",
      "epoch: 998, train_loss: 0.1277260626291052 \n",
      "epoch: 999, train_loss: 0.12771691862203277 \n",
      "epoch: 1000, train_loss: 0.1277070683765865 \n",
      "epoch: 1001, train_loss: 0.12769717493566085 \n",
      "epoch: 1002, train_loss: 0.1276886306973431 \n",
      "epoch: 1003, train_loss: 0.1276781483890528 \n",
      "epoch: 1004, train_loss: 0.12767043486203358 \n",
      "epoch: 1005, train_loss: 0.12765944325785805 \n",
      "epoch: 1006, train_loss: 0.12765232304819205 \n",
      "epoch: 1007, train_loss: 0.12764089134887205 \n",
      "epoch: 1008, train_loss: 0.1276342472776537 \n",
      "epoch: 1009, train_loss: 0.12762242533954032 \n",
      "epoch: 1010, train_loss: 0.12761617447467105 \n",
      "epoch: 1011, train_loss: 0.12760393293874475 \n",
      "epoch: 1012, train_loss: 0.1275980958486671 \n",
      "epoch: 1013, train_loss: 0.12758539621045933 \n",
      "epoch: 1014, train_loss: 0.1275799801174799 \n",
      "epoch: 1015, train_loss: 0.12756672344832737 \n",
      "epoch: 1016, train_loss: 0.12756182442299024 \n",
      "epoch: 1017, train_loss: 0.1275480015898138 \n",
      "epoch: 1018, train_loss: 0.12754360042903318 \n",
      "epoch: 1019, train_loss: 0.12752905509758683 \n",
      "epoch: 1020, train_loss: 0.12752527279514933 \n",
      "epoch: 1021, train_loss: 0.1275100083883232 \n",
      "epoch: 1022, train_loss: 0.12750681187836138 \n",
      "epoch: 1023, train_loss: 0.12749024219764174 \n",
      "epoch: 1024, train_loss: 0.12748806304390786 \n",
      "epoch: 1025, train_loss: 0.12746946345491175 \n",
      "epoch: 1026, train_loss: 0.1274695162805988 \n",
      "epoch: 1027, train_loss: 0.12745067856444348 \n",
      "epoch: 1028, train_loss: 0.12745164315445665 \n",
      "epoch: 1029, train_loss: 0.12743311145946423 \n",
      "epoch: 1030, train_loss: 0.12742882040857179 \n",
      "epoch: 1031, train_loss: 0.12707484690056275 \n",
      "epoch: 1032, train_loss: 0.12690973261126323 \n",
      "epoch: 1033, train_loss: 0.12703974420338643 \n",
      "epoch: 1034, train_loss: 0.12732657507432307 \n",
      "epoch: 1035, train_loss: 0.1274028248062077 \n",
      "epoch: 1036, train_loss: 0.12741340945268403 \n",
      "epoch: 1037, train_loss: 0.12737277873712938 \n",
      "epoch: 1038, train_loss: 0.12736235704302906 \n",
      "epoch: 1039, train_loss: 0.12736455531497454 \n",
      "epoch: 1040, train_loss: 0.12737819764677505 \n",
      "epoch: 1041, train_loss: 0.12733650442455074 \n",
      "epoch: 1042, train_loss: 0.12706193427995083 \n",
      "epoch: 1043, train_loss: 0.1272552921518349 \n",
      "epoch: 1044, train_loss: 0.12729260405853784 \n",
      "epoch: 1045, train_loss: 0.12731663071479146 \n",
      "epoch: 1046, train_loss: 0.12729877011040297 \n",
      "epoch: 1047, train_loss: 0.12728936465771828 \n",
      "epoch: 1048, train_loss: 0.12724570628324347 \n",
      "epoch: 1049, train_loss: 0.12725974392731848 \n",
      "epoch: 1050, train_loss: 0.12726476861879046 \n",
      "epoch: 1051, train_loss: 0.12719248771375763 \n",
      "epoch: 1052, train_loss: 0.12716222406310965 \n",
      "epoch: 1053, train_loss: 0.1272242664592473 \n",
      "epoch: 1054, train_loss: 0.1272375935010819 \n",
      "epoch: 1055, train_loss: 0.12723762819946824 \n",
      "epoch: 1056, train_loss: 0.12722102951948971 \n",
      "epoch: 1057, train_loss: 0.12720941372193692 \n",
      "epoch: 1058, train_loss: 0.12720002400641145 \n",
      "epoch: 1059, train_loss: 0.12719019184033717 \n",
      "epoch: 1060, train_loss: 0.1271801719220103 \n",
      "epoch: 1061, train_loss: 0.12716985079210855 \n",
      "epoch: 1062, train_loss: 0.12715918297112777 \n",
      "epoch: 1063, train_loss: 0.1271480504581557 \n",
      "epoch: 1064, train_loss: 0.12713606016210371 \n",
      "epoch: 1065, train_loss: 0.12712061070813202 \n",
      "epoch: 1066, train_loss: 0.12700967199770782 \n",
      "epoch: 1067, train_loss: 0.1269536760278199 \n",
      "epoch: 1068, train_loss: 0.12707683114955967 \n",
      "epoch: 1069, train_loss: 0.12709681072434614 \n",
      "epoch: 1070, train_loss: 0.12708669634017064 \n",
      "epoch: 1071, train_loss: 0.12707628671742632 \n",
      "epoch: 1072, train_loss: 0.12706562644060349 \n",
      "epoch: 1073, train_loss: 0.12705468166055697 \n",
      "epoch: 1074, train_loss: 0.12704337807452185 \n",
      "epoch: 1075, train_loss: 0.12703158136458984 \n",
      "epoch: 1076, train_loss: 0.12701900467587682 \n",
      "epoch: 1077, train_loss: 0.12700462593186518 \n",
      "epoch: 1078, train_loss: 0.12698470793383795 \n",
      "epoch: 1079, train_loss: 0.12687274118112804 \n",
      "epoch: 1080, train_loss: 0.12696020504740319 \n",
      "epoch: 1081, train_loss: 0.12683985876007878 \n",
      "epoch: 1082, train_loss: 0.12693834810001076 \n",
      "epoch: 1083, train_loss: 0.1269731378902886 \n",
      "epoch: 1084, train_loss: 0.12690097512104173 \n",
      "epoch: 1085, train_loss: 0.12679849720765288 \n",
      "epoch: 1086, train_loss: 0.1268898631783626 \n",
      "epoch: 1087, train_loss: 0.12691453400502292 \n",
      "epoch: 1088, train_loss: 0.12692045131022053 \n",
      "epoch: 1089, train_loss: 0.1268327563332204 \n",
      "epoch: 1090, train_loss: 0.12676261590040466 \n",
      "epoch: 1091, train_loss: 0.12642448527635927 \n",
      "epoch: 1092, train_loss: 0.12636887630620058 \n",
      "epoch: 1093, train_loss: 0.12636324213469768 \n",
      "epoch: 1094, train_loss: 0.12634887590810273 \n",
      "epoch: 1095, train_loss: 0.12633737735492376 \n",
      "epoch: 1096, train_loss: 0.12632787154338032 \n",
      "epoch: 1097, train_loss: 0.12631916864945347 \n",
      "epoch: 1098, train_loss: 0.12631072090550452 \n",
      "epoch: 1099, train_loss: 0.12630238435395214 \n",
      "epoch: 1100, train_loss: 0.12629411501479776 \n",
      "epoch: 1101, train_loss: 0.12628588754672965 \n",
      "epoch: 1102, train_loss: 0.1262776817190637 \n",
      "epoch: 1103, train_loss: 0.12626948040552613 \n",
      "epoch: 1104, train_loss: 0.12626126916330913 \n",
      "epoch: 1105, train_loss: 0.12625303600237747 \n",
      "epoch: 1106, train_loss: 0.12624477117292252 \n",
      "epoch: 1107, train_loss: 0.1262364669509445 \n",
      "epoch: 1108, train_loss: 0.1262281174203241 \n",
      "epoch: 1109, train_loss: 0.12621971825411438 \n",
      "epoch: 1110, train_loss: 0.126211266500007 \n",
      "epoch: 1111, train_loss: 0.1262027603754632 \n",
      "epoch: 1112, train_loss: 0.12619419907714097 \n",
      "epoch: 1113, train_loss: 0.12618558260776827 \n",
      "epoch: 1114, train_loss: 0.12617691162203612 \n",
      "epoch: 1115, train_loss: 0.12616818729179846 \n",
      "epoch: 1116, train_loss: 0.1261594111899165 \n",
      "epoch: 1117, train_loss: 0.12615058519151232 \n",
      "epoch: 1118, train_loss: 0.12614171139105665 \n",
      "epoch: 1119, train_loss: 0.1261327920335917 \n",
      "epoch: 1120, train_loss: 0.12612382945837264 \n",
      "epoch: 1121, train_loss: 0.12611482605328622 \n",
      "epoch: 1122, train_loss: 0.12610578421851454 \n",
      "epoch: 1123, train_loss: 0.12609670633804365 \n",
      "epoch: 1124, train_loss: 0.12608759475776066 \n",
      "epoch: 1125, train_loss: 0.1260784517690186 \n",
      "epoch: 1126, train_loss: 0.1260692795966743 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1127, train_loss: 0.12606008039072766 \n",
      "epoch: 1128, train_loss: 0.12605085622077808 \n",
      "epoch: 1129, train_loss: 0.12604160907261333 \n",
      "epoch: 1130, train_loss: 0.12603234084628928 \n",
      "epoch: 1131, train_loss: 0.1260230533551257 \n",
      "epoch: 1132, train_loss: 0.12601374832504988 \n",
      "epoch: 1133, train_loss: 0.126004427393741 \n",
      "epoch: 1134, train_loss: 0.12599509210899656 \n",
      "epoch: 1135, train_loss: 0.12598574392573483 \n",
      "epoch: 1136, train_loss: 0.125976384200987 \n",
      "epoch: 1137, train_loss: 0.12596701418620368 \n",
      "epoch: 1138, train_loss: 0.125957635016183 \n",
      "epoch: 1139, train_loss: 0.1259482476939348 \n",
      "epoch: 1140, train_loss: 0.1259388530709328 \n",
      "epoch: 1141, train_loss: 0.12592945182245235 \n",
      "epoch: 1142, train_loss: 0.12592004441818702 \n",
      "epoch: 1143, train_loss: 0.12591063108911735 \n",
      "epoch: 1144, train_loss: 0.12590121179275177 \n",
      "epoch: 1145, train_loss: 0.12589178618035765 \n",
      "epoch: 1146, train_loss: 0.12588235357151056 \n",
      "epoch: 1147, train_loss: 0.12587291294288075 \n",
      "epoch: 1148, train_loss: 0.12586346293907 \n",
      "epoch: 1149, train_loss: 0.12585400191274454 \n",
      "epoch: 1150, train_loss: 0.12584452799854726 \n",
      "epoch: 1151, train_loss: 0.12583503922004588 \n",
      "epoch: 1152, train_loss: 0.12582553362189075 \n",
      "epoch: 1153, train_loss: 0.12581600941211338 \n",
      "epoch: 1154, train_loss: 0.12580646509463012 \n",
      "epoch: 1155, train_loss: 0.12579689957176549 \n",
      "epoch: 1156, train_loss: 0.1257873122017945 \n",
      "epoch: 1157, train_loss: 0.12577770280580416 \n",
      "epoch: 1158, train_loss: 0.1257680716285237 \n",
      "epoch: 1159, train_loss: 0.12575841926584494 \n",
      "epoch: 1160, train_loss: 0.12574874657539803 \n",
      "epoch: 1161, train_loss: 0.12573905458563508 \n",
      "epoch: 1162, train_loss: 0.1257293444148278 \n",
      "epoch: 1163, train_loss: 0.1257196172062141 \n",
      "epoch: 1164, train_loss: 0.12570987408094822 \n",
      "epoch: 1165, train_loss: 0.12570011610736645 \n",
      "epoch: 1166, train_loss: 0.12569034428347733 \n",
      "epoch: 1167, train_loss: 0.1256805595292051 \n",
      "epoch: 1168, train_loss: 0.12567076268524655 \n",
      "epoch: 1169, train_loss: 0.12566095451605233 \n",
      "epoch: 1170, train_loss: 0.12565113571512107 \n",
      "epoch: 1171, train_loss: 0.12564130691138672 \n",
      "epoch: 1172, train_loss: 0.1256314686759315 \n",
      "epoch: 1173, train_loss: 0.1256216215285598 \n",
      "epoch: 1174, train_loss: 0.12561176594398452 \n",
      "epoch: 1175, train_loss: 0.1256019023575123 \n",
      "epoch: 1176, train_loss: 0.12559203117017534 \n",
      "epoch: 1177, train_loss: 0.1255821527533239 \n",
      "epoch: 1178, train_loss: 0.12557226745270314 \n",
      "epoch: 1179, train_loss: 0.12556237559205127 \n",
      "epoch: 1180, train_loss: 0.12555247747626713 \n",
      "epoch: 1181, train_loss: 0.12554257339418706 \n",
      "epoch: 1182, train_loss: 0.1255326636210096 \n",
      "epoch: 1183, train_loss: 0.12552274842041122 \n",
      "epoch: 1184, train_loss: 0.12551282804637703 \n",
      "epoch: 1185, train_loss: 0.12550290274478654 \n",
      "epoch: 1186, train_loss: 0.12549297275477006 \n",
      "epoch: 1187, train_loss: 0.12548303830986646 \n",
      "epoch: 1188, train_loss: 0.12547309963899622 \n",
      "epoch: 1189, train_loss: 0.12546315696727117 \n",
      "epoch: 1190, train_loss: 0.12545321051665365 \n",
      "epoch: 1191, train_loss: 0.12544326050648041 \n",
      "epoch: 1192, train_loss: 0.12543330715386083 \n",
      "epoch: 1193, train_loss: 0.1254233506739623 \n",
      "epoch: 1194, train_loss: 0.12541339128019058 \n",
      "epoch: 1195, train_loss: 0.12540342918427744 \n",
      "epoch: 1196, train_loss: 0.125393464596278 \n",
      "epoch: 1197, train_loss: 0.1253834977244911 \n",
      "epoch: 1198, train_loss: 0.1253735287753058 \n",
      "epoch: 1199, train_loss: 0.12536355795298457 \n",
      "epoch: 1200, train_loss: 0.12535358545938685 \n",
      "epoch: 1201, train_loss: 0.12534361149364112 \n",
      "epoch: 1202, train_loss: 0.12533363625177166 \n",
      "epoch: 1203, train_loss: 0.1253236599262845 \n",
      "epoch: 1204, train_loss: 0.1253136827057239 \n",
      "epoch: 1205, train_loss: 0.12530370477419794 \n",
      "epoch: 1206, train_loss: 0.12529372631088398 \n",
      "epoch: 1207, train_loss: 0.1252837474895203 \n",
      "epoch: 1208, train_loss: 0.12527376847788319 \n",
      "epoch: 1209, train_loss: 0.12526378943726316 \n",
      "epoch: 1210, train_loss: 0.1252538105219354 \n",
      "epoch: 1211, train_loss: 0.12524383187863847 \n",
      "epoch: 1212, train_loss: 0.125233853646055 \n",
      "epoch: 1213, train_loss: 0.12522387595430953 \n",
      "epoch: 1214, train_loss: 0.12521389892447138 \n",
      "epoch: 1215, train_loss: 0.12520392266808178 \n",
      "epoch: 1216, train_loss: 0.12519394728669278 \n",
      "epoch: 1217, train_loss: 0.12518397287143093 \n",
      "epoch: 1218, train_loss: 0.12517399950257665 \n",
      "epoch: 1219, train_loss: 0.1251640272491715 \n",
      "epoch: 1220, train_loss: 0.12515405616864322 \n",
      "epoch: 1221, train_loss: 0.1251440863064566 \n",
      "epoch: 1222, train_loss: 0.1251341176957912 \n",
      "epoch: 1223, train_loss: 0.12512415035723737 \n",
      "epoch: 1224, train_loss: 0.12511418429852567 \n",
      "epoch: 1225, train_loss: 0.12510421951427467 \n",
      "epoch: 1226, train_loss: 0.12509425598577326 \n",
      "epoch: 1227, train_loss: 0.12508429368078502 \n",
      "epoch: 1228, train_loss: 0.12507433255338318 \n",
      "epoch: 1229, train_loss: 0.12506437254381564 \n",
      "epoch: 1230, train_loss: 0.125054413578396 \n",
      "epoch: 1231, train_loss: 0.12504445556942592 \n",
      "epoch: 1232, train_loss: 0.1250344984151404 \n",
      "epoch: 1233, train_loss: 0.12502454199967653 \n",
      "epoch: 1234, train_loss: 0.12501458619306 \n",
      "epoch: 1235, train_loss: 0.12500463085119715 \n",
      "epoch: 1236, train_loss: 0.12499467581586413 \n",
      "epoch: 1237, train_loss: 0.12498472091467482 \n",
      "epoch: 1238, train_loss: 0.12497476596100633 \n",
      "epoch: 1239, train_loss: 0.12496481075385449 \n",
      "epoch: 1240, train_loss: 0.12495485507758723 \n",
      "epoch: 1241, train_loss: 0.12494489870154823 \n",
      "epoch: 1242, train_loss: 0.12493494137947056 \n",
      "epoch: 1243, train_loss: 0.12492498284863507 \n",
      "epoch: 1244, train_loss: 0.1249150228287081 \n",
      "epoch: 1245, train_loss: 0.12490506102018736 \n",
      "epoch: 1246, train_loss: 0.12489509710237487 \n",
      "epoch: 1247, train_loss: 0.12488513073078134 \n",
      "epoch: 1248, train_loss: 0.12487516153388485 \n",
      "epoch: 1249, train_loss: 0.12486518910914338 \n",
      "epoch: 1250, train_loss: 0.1248552130181801 \n",
      "epoch: 1251, train_loss: 0.12484523278107205 \n",
      "epoch: 1252, train_loss: 0.12483524786968068 \n",
      "epoch: 1253, train_loss: 0.12482525770000669 \n",
      "epoch: 1254, train_loss: 0.12481526162358172 \n",
      "epoch: 1255, train_loss: 0.1248052589179661 \n",
      "epoch: 1256, train_loss: 0.12479524877649704 \n",
      "epoch: 1257, train_loss: 0.12478523029750903 \n",
      "epoch: 1258, train_loss: 0.12477520247335487 \n",
      "epoch: 1259, train_loss: 0.12476516417968089 \n",
      "epoch: 1260, train_loss: 0.12475511416552745 \n",
      "epoch: 1261, train_loss: 0.12474505104498634 \n",
      "epoch: 1262, train_loss: 0.12473497329126845 \n",
      "epoch: 1263, train_loss: 0.12472487923418371 \n",
      "epoch: 1264, train_loss: 0.12471476706212575 \n",
      "epoch: 1265, train_loss: 0.12470463482973176 \n",
      "epoch: 1266, train_loss: 0.12469448047237112 \n",
      "epoch: 1267, train_loss: 0.12468430182854237 \n",
      "epoch: 1268, train_loss: 0.12467409667105794 \n",
      "epoch: 1269, train_loss: 0.12466386274759103 \n",
      "epoch: 1270, train_loss: 0.12465359783072033 \n",
      "epoch: 1271, train_loss: 0.1246432997770537 \n",
      "epoch: 1272, train_loss: 0.12463296659436676 \n",
      "epoch: 1273, train_loss: 0.12462259651497685 \n",
      "epoch: 1274, train_loss: 0.1246121880728723 \n",
      "epoch: 1275, train_loss: 0.12460174018145836 \n",
      "epoch: 1276, train_loss: 0.12459125220827802 \n",
      "epoch: 1277, train_loss: 0.12458072404273376 \n",
      "epoch: 1278, train_loss: 0.12457015615278 \n",
      "epoch: 1279, train_loss: 0.12455954962676087 \n",
      "epoch: 1280, train_loss: 0.1245489061970629 \n",
      "epoch: 1281, train_loss: 0.12453822824298776 \n",
      "epoch: 1282, train_loss: 0.12452751877118765 \n",
      "epoch: 1283, train_loss: 0.1245167813730446 \n",
      "epoch: 1284, train_loss: 0.12450602015942872 \n",
      "epoch: 1285, train_loss: 0.12449523967424958 \n",
      "epoch: 1286, train_loss: 0.12448444478902122 \n",
      "epoch: 1287, train_loss: 0.12447364058125351 \n",
      "epoch: 1288, train_loss: 0.12446283219984282 \n",
      "epoch: 1289, train_loss: 0.12445202472076813 \n",
      "epoch: 1290, train_loss: 0.12444122299642577 \n",
      "epoch: 1291, train_loss: 0.12443043150194355 \n",
      "epoch: 1292, train_loss: 0.12441965418198682 \n",
      "epoch: 1293, train_loss: 0.12440889430204435 \n",
      "epoch: 1294, train_loss: 0.12439815430909325 \n",
      "epoch: 1295, train_loss: 0.1243874357078775 \n",
      "epoch: 1296, train_loss: 0.12437673896064994 \n",
      "epoch: 1297, train_loss: 0.12436606341973282 \n",
      "epoch: 1298, train_loss: 0.12435540730310032 \n",
      "epoch: 1299, train_loss: 0.12434476772264817 \n",
      "epoch: 1300, train_loss: 0.12433414077229848 \n",
      "epoch: 1301, train_loss: 0.12432352167832879 \n",
      "epoch: 1302, train_loss: 0.12431290500755703 \n",
      "epoch: 1303, train_loss: 0.1243022849213474 \n",
      "epoch: 1304, train_loss: 0.12429165545628634 \n",
      "epoch: 1305, train_loss: 0.12428101080763992 \n",
      "epoch: 1306, train_loss: 0.1242703455905658 \n",
      "epoch: 1307, train_loss: 0.12425965505706402 \n",
      "epoch: 1308, train_loss: 0.1242489352531028 \n",
      "epoch: 1309, train_loss: 0.12423818310881453 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1310, train_loss: 0.12422739646322536 \n",
      "epoch: 1311, train_loss: 0.12421657403203773 \n",
      "epoch: 1312, train_loss: 0.12420571533149731 \n",
      "epoch: 1313, train_loss: 0.1241948205731358 \n",
      "epoch: 1314, train_loss: 0.12418389054354605 \n",
      "epoch: 1315, train_loss: 0.12417292648113765 \n",
      "epoch: 1316, train_loss: 0.12416192995880194 \n",
      "epoch: 1317, train_loss: 0.12415090277832673 \n",
      "epoch: 1318, train_loss: 0.12413984687967292 \n",
      "epoch: 1319, train_loss: 0.12412876426609531 \n",
      "epoch: 1320, train_loss: 0.12411765694460133 \n",
      "epoch: 1321, train_loss: 0.12410652688032008 \n",
      "epoch: 1322, train_loss: 0.12409537596289545 \n",
      "epoch: 1323, train_loss: 0.12408420598288228 \n",
      "epoch: 1324, train_loss: 0.1240730186161829 \n",
      "epoch: 1325, train_loss: 0.12406181541477072 \n",
      "epoch: 1326, train_loss: 0.12405059780217374 \n",
      "epoch: 1327, train_loss: 0.12403936707245769 \n",
      "epoch: 1328, train_loss: 0.12402812439168467 \n",
      "epoch: 1329, train_loss: 0.12401687080103756 \n",
      "epoch: 1330, train_loss: 0.12400560722098292 \n",
      "epoch: 1331, train_loss: 0.12399433445599442 \n",
      "epoch: 1332, train_loss: 0.12398305319947912 \n",
      "epoch: 1333, train_loss: 0.12397176403863605 \n",
      "epoch: 1334, train_loss: 0.12396046745906285 \n",
      "epoch: 1335, train_loss: 0.1239491638489657 \n",
      "epoch: 1336, train_loss: 0.1239378535028728 \n",
      "epoch: 1337, train_loss: 0.12392653662478859 \n",
      "epoch: 1338, train_loss: 0.12391521333072768 \n",
      "epoch: 1339, train_loss: 0.12390388365060168 \n",
      "epoch: 1340, train_loss: 0.12389254752941946 \n",
      "epoch: 1341, train_loss: 0.12388120482778037 \n",
      "epoch: 1342, train_loss: 0.12386985532163117 \n",
      "epoch: 1343, train_loss: 0.12385849870125532 \n",
      "epoch: 1344, train_loss: 0.12384713456946067 \n",
      "epoch: 1345, train_loss: 0.12383576243892613 \n",
      "epoch: 1346, train_loss: 0.12382438172864987 \n",
      "epoch: 1347, train_loss: 0.12381299175944376 \n",
      "epoch: 1348, train_loss: 0.12380159174839513 \n",
      "epoch: 1349, train_loss: 0.12379018080220691 \n",
      "epoch: 1350, train_loss: 0.12377875790930892 \n",
      "epoch: 1351, train_loss: 0.12376732193061647 \n",
      "epoch: 1352, train_loss: 0.12375587158877958 \n",
      "epoch: 1353, train_loss: 0.12374440545575242 \n",
      "epoch: 1354, train_loss: 0.12373292193846931 \n",
      "epoch: 1355, train_loss: 0.12372141926238178 \n",
      "epoch: 1356, train_loss: 0.12370989545256533 \n",
      "epoch: 1357, train_loss: 0.12369834831205151 \n",
      "epoch: 1358, train_loss: 0.12368677539697723 \n",
      "epoch: 1359, train_loss: 0.1236751739880658 \n",
      "epoch: 1360, train_loss: 0.12366354105786238 \n",
      "epoch: 1361, train_loss: 0.12365187323303133 \n",
      "epoch: 1362, train_loss: 0.12364016675088754 \n",
      "epoch: 1363, train_loss: 0.1236284174091566 \n",
      "epoch: 1364, train_loss: 0.123616620507756 \n",
      "epoch: 1365, train_loss: 0.123604770781124 \n",
      "epoch: 1366, train_loss: 0.12359286231929771 \n",
      "epoch: 1367, train_loss: 0.1235808884755409 \n",
      "epoch: 1368, train_loss: 0.12356884175781004 \n",
      "epoch: 1369, train_loss: 0.12355671370071086 \n",
      "epoch: 1370, train_loss: 0.12354449471379422 \n",
      "epoch: 1371, train_loss: 0.12353217390100156 \n",
      "epoch: 1372, train_loss: 0.12351973884478154 \n",
      "epoch: 1373, train_loss: 0.12350717534669799 \n",
      "epoch: 1374, train_loss: 0.12349446711418617 \n",
      "epoch: 1375, train_loss: 0.12348159538028433 \n",
      "epoch: 1376, train_loss: 0.12346853843945474 \n",
      "epoch: 1377, train_loss: 0.1234552710777037 \n",
      "epoch: 1378, train_loss: 0.12344176386868357 \n",
      "epoch: 1379, train_loss: 0.12342798229867255 \n",
      "epoch: 1380, train_loss: 0.12341388567140768 \n",
      "epoch: 1381, train_loss: 0.12339942572741369 \n",
      "epoch: 1382, train_loss: 0.1233845448898199 \n",
      "epoch: 1383, train_loss: 0.12336917401692374 \n",
      "epoch: 1384, train_loss: 0.12335322949667878 \n",
      "epoch: 1385, train_loss: 0.1233366094534009 \n",
      "epoch: 1386, train_loss: 0.12331918874215043 \n",
      "epoch: 1387, train_loss: 0.12330081226538299 \n",
      "epoch: 1388, train_loss: 0.12328128593340486 \n",
      "epoch: 1389, train_loss: 0.12326036426169287 \n",
      "epoch: 1390, train_loss: 0.12323773308118291 \n",
      "epoch: 1391, train_loss: 0.1232129850062389 \n",
      "epoch: 1392, train_loss: 0.12318558393866466 \n",
      "epoch: 1393, train_loss: 0.12315481259463136 \n",
      "epoch: 1394, train_loss: 0.12311969313869192 \n",
      "epoch: 1395, train_loss: 0.1230788643413223 \n",
      "epoch: 1396, train_loss: 0.12303038760087551 \n",
      "epoch: 1397, train_loss: 0.12297143790301794 \n",
      "epoch: 1398, train_loss: 0.12289782330513312 \n",
      "epoch: 1399, train_loss: 0.12280333186740994 \n",
      "epoch: 1400, train_loss: 0.12267934596545028 \n",
      "epoch: 1401, train_loss: 0.12251717155055787 \n",
      "epoch: 1402, train_loss: 0.12232051646839816 \n",
      "epoch: 1403, train_loss: 0.12212902938124082 \n",
      "epoch: 1404, train_loss: 0.12200266838289643 \n",
      "epoch: 1405, train_loss: 0.12195392668045324 \n",
      "epoch: 1406, train_loss: 0.12195153797433218 \n",
      "epoch: 1407, train_loss: 0.12196830936298123 \n",
      "epoch: 1408, train_loss: 0.12199157320499629 \n",
      "epoch: 1409, train_loss: 0.12201649040438521 \n",
      "epoch: 1410, train_loss: 0.1220412888114899 \n",
      "epoch: 1411, train_loss: 0.12206531988896761 \n",
      "epoch: 1412, train_loss: 0.12208835349014545 \n",
      "epoch: 1413, train_loss: 0.1221103216909286 \n",
      "epoch: 1414, train_loss: 0.12213122202082007 \n",
      "epoch: 1415, train_loss: 0.12215107897440587 \n",
      "epoch: 1416, train_loss: 0.1221699278809324 \n",
      "epoch: 1417, train_loss: 0.12218780782114588 \n",
      "epoch: 1418, train_loss: 0.12220475841292894 \n",
      "epoch: 1419, train_loss: 0.12222081834031437 \n",
      "epoch: 1420, train_loss: 0.12223602470586266 \n",
      "epoch: 1421, train_loss: 0.12225041278617964 \n",
      "epoch: 1422, train_loss: 0.12226401598875858 \n",
      "epoch: 1423, train_loss: 0.12227686590869552 \n",
      "epoch: 1424, train_loss: 0.12228899243236302 \n",
      "epoch: 1425, train_loss: 0.12230042385970404 \n",
      "epoch: 1426, train_loss: 0.1223111870297773 \n",
      "epoch: 1427, train_loss: 0.12232130744127089 \n",
      "epoch: 1428, train_loss: 0.12233080936365567 \n",
      "epoch: 1429, train_loss: 0.1223397159368771 \n",
      "epoch: 1430, train_loss: 0.12234804925874365 \n",
      "epoch: 1431, train_loss: 0.12235583045984764 \n",
      "epoch: 1432, train_loss: 0.12236307976619103 \n",
      "epoch: 1433, train_loss: 0.12236981654982888 \n",
      "epoch: 1434, train_loss: 0.12237605936786476 \n",
      "epoch: 1435, train_loss: 0.12238182599010684 \n",
      "epoch: 1436, train_loss: 0.12238713341568142 \n",
      "epoch: 1437, train_loss: 0.12239199787893484 \n",
      "epoch: 1438, train_loss: 0.12239643484510591 \n",
      "epoch: 1439, train_loss: 0.12240045899646781 \n",
      "epoch: 1440, train_loss: 0.12240408420989332 \n",
      "epoch: 1441, train_loss: 0.1224073235268298 \n",
      "epoch: 1442, train_loss: 0.12241018911615412 \n",
      "epoch: 1443, train_loss: 0.12241269222888873 \n",
      "epoch: 1444, train_loss: 0.12241484314100587 \n",
      "epoch: 1445, train_loss: 0.12241665107687671 \n",
      "epoch: 1446, train_loss: 0.12241812410269166 \n",
      "epoch: 1447, train_loss: 0.12241926897902312 \n",
      "epoch: 1448, train_loss: 0.12242009096807688 \n",
      "epoch: 1449, train_loss: 0.12242059360710107 \n",
      "epoch: 1450, train_loss: 0.1224207784852089 \n",
      "epoch: 1451, train_loss: 0.12242064509174623 \n",
      "epoch: 1452, train_loss: 0.12242019082794317 \n",
      "epoch: 1453, train_loss: 0.12241941127027184 \n",
      "epoch: 1454, train_loss: 0.12241830072406126 \n",
      "epoch: 1455, train_loss: 0.12241685300749908 \n",
      "epoch: 1456, train_loss: 0.12241506229458805 \n",
      "epoch: 1457, train_loss: 0.12241292378866157 \n",
      "epoch: 1458, train_loss: 0.12241043404755171 \n",
      "epoch: 1459, train_loss: 0.1224075909162633 \n",
      "epoch: 1460, train_loss: 0.12240439315619908 \n",
      "epoch: 1461, train_loss: 0.12240083991836528 \n",
      "epoch: 1462, train_loss: 0.12239693019217723 \n",
      "epoch: 1463, train_loss: 0.12239266232669083 \n",
      "epoch: 1464, train_loss: 0.12238803372854835 \n",
      "epoch: 1465, train_loss: 0.12238304093371959 \n",
      "epoch: 1466, train_loss: 0.12237768041987337 \n",
      "epoch: 1467, train_loss: 0.12237195060117263 \n",
      "epoch: 1468, train_loss: 0.12236585500601344 \n",
      "epoch: 1469, train_loss: 0.12235940548753475 \n",
      "epoch: 1470, train_loss: 0.12235262365384753 \n",
      "epoch: 1471, train_loss: 0.1223455404583932 \n",
      "epoch: 1472, train_loss: 0.12233819685336604 \n",
      "epoch: 1473, train_loss: 0.12233064872726998 \n",
      "epoch: 1474, train_loss: 0.12232297687893871 \n",
      "epoch: 1475, train_loss: 0.12231529627765486 \n",
      "epoch: 1476, train_loss: 0.1223077268363601 \n",
      "epoch: 1477, train_loss: 0.12230022449080964 \n",
      "epoch: 1478, train_loss: 0.12229243337818103 \n",
      "epoch: 1479, train_loss: 0.12228437161978017 \n",
      "epoch: 1480, train_loss: 0.12227694600110339 \n",
      "epoch: 1481, train_loss: 0.12227457585325587 \n",
      "epoch: 1482, train_loss: 0.12226089837288695 \n",
      "epoch: 1483, train_loss: 0.12231006031129685 \n",
      "epoch: 1484, train_loss: 0.1223500838168714 \n",
      "epoch: 1485, train_loss: 0.1223730350770039 \n",
      "epoch: 1486, train_loss: 0.12224866143064834 \n",
      "epoch: 1487, train_loss: 0.12226236231253305 \n",
      "epoch: 1488, train_loss: 0.12225321085972794 \n",
      "epoch: 1489, train_loss: 0.12224956431490115 \n",
      "epoch: 1490, train_loss: 0.12234970087550918 \n",
      "epoch: 1491, train_loss: 0.1223648110083941 \n",
      "epoch: 1492, train_loss: 0.12226585961190359 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1493, train_loss: 0.12227313761430274 \n",
      "epoch: 1494, train_loss: 0.12235492408842064 \n",
      "epoch: 1495, train_loss: 0.12235240365809638 \n",
      "epoch: 1496, train_loss: 0.12224696414323385 \n",
      "epoch: 1497, train_loss: 0.12223900846279784 \n",
      "epoch: 1498, train_loss: 0.12225061836777051 \n",
      "epoch: 1499, train_loss: 0.1223313485237744 \n",
      "epoch: 1500, train_loss: 0.12223832177192859 \n",
      "epoch: 1501, train_loss: 0.12231462875961023 \n",
      "epoch: 1502, train_loss: 0.12225728559680843 \n",
      "epoch: 1503, train_loss: 0.12233890111196834 \n",
      "epoch: 1504, train_loss: 0.12219873323130383 \n",
      "epoch: 1505, train_loss: 0.12232334230305025 \n",
      "epoch: 1506, train_loss: 0.12231185783091465 \n",
      "epoch: 1507, train_loss: 0.12236168291484317 \n",
      "epoch: 1508, train_loss: 0.12234517442063368 \n",
      "epoch: 1509, train_loss: 0.12223270435285749 \n",
      "epoch: 1510, train_loss: 0.1223765990352466 \n",
      "epoch: 1511, train_loss: 0.12234401826268625 \n",
      "epoch: 1512, train_loss: 0.12219690713653429 \n",
      "epoch: 1513, train_loss: 0.12229811303225117 \n",
      "epoch: 1514, train_loss: 0.12221318551030318 \n",
      "epoch: 1515, train_loss: 0.12234464956459207 \n",
      "epoch: 1516, train_loss: 0.12230977052833085 \n",
      "epoch: 1517, train_loss: 0.12233838246751697 \n",
      "epoch: 1518, train_loss: 0.1222222824720587 \n",
      "epoch: 1519, train_loss: 0.12234797720418622 \n",
      "epoch: 1520, train_loss: 0.12229521161734519 \n",
      "epoch: 1521, train_loss: 0.12250056265161957 \n",
      "epoch: 1522, train_loss: 0.12237335530306272 \n",
      "epoch: 1523, train_loss: 0.12241690909372585 \n",
      "epoch: 1524, train_loss: 0.1226598054262463 \n",
      "epoch: 1525, train_loss: 0.12226167176950224 \n",
      "epoch: 1526, train_loss: 0.12241807343682862 \n",
      "epoch: 1527, train_loss: 0.1224805076211457 \n",
      "epoch: 1528, train_loss: 0.12238327408055044 \n",
      "epoch: 1529, train_loss: 0.12238429805733618 \n",
      "epoch: 1530, train_loss: 0.12247209062674709 \n",
      "epoch: 1531, train_loss: 0.12250741643888169 \n",
      "epoch: 1532, train_loss: 0.12237369704519419 \n",
      "epoch: 1533, train_loss: 0.1223838372212825 \n",
      "epoch: 1534, train_loss: 0.1223981307735789 \n",
      "epoch: 1535, train_loss: 0.12240793818005614 \n",
      "epoch: 1536, train_loss: 0.12240723462276747 \n",
      "epoch: 1537, train_loss: 0.1224049350034738 \n",
      "epoch: 1538, train_loss: 0.1225756387799979 \n",
      "epoch: 1539, train_loss: 0.12234082687048907 \n",
      "epoch: 1540, train_loss: 0.12234717950549998 \n",
      "epoch: 1541, train_loss: 0.1222710084854023 \n",
      "epoch: 1542, train_loss: 0.12233454220167396 \n",
      "epoch: 1543, train_loss: 0.12231190511319553 \n",
      "epoch: 1544, train_loss: 0.12235435386556957 \n",
      "epoch: 1545, train_loss: 0.12240503879167237 \n",
      "epoch: 1546, train_loss: 0.12228899649020479 \n",
      "epoch: 1547, train_loss: 0.12248798216262485 \n",
      "epoch: 1548, train_loss: 0.1223292915854818 \n",
      "epoch: 1549, train_loss: 0.12245441704517777 \n",
      "epoch: 1550, train_loss: 0.12228963056922694 \n",
      "epoch: 1551, train_loss: 0.12253596100615499 \n",
      "epoch: 1552, train_loss: 0.12236242424248092 \n",
      "epoch: 1553, train_loss: 0.12252410479896478 \n",
      "epoch: 1554, train_loss: 0.12228446512298771 \n",
      "epoch: 1555, train_loss: 0.12256534630617213 \n",
      "epoch: 1556, train_loss: 0.12230283281522862 \n",
      "epoch: 1557, train_loss: 0.12250848516473999 \n",
      "epoch: 1558, train_loss: 0.12227305950815817 \n",
      "epoch: 1559, train_loss: 0.12250061405687548 \n",
      "epoch: 1560, train_loss: 0.1223010200572479 \n",
      "epoch: 1561, train_loss: 0.12249927287040675 \n",
      "epoch: 1562, train_loss: 0.12228658440114507 \n",
      "epoch: 1563, train_loss: 0.12243337088848195 \n",
      "epoch: 1564, train_loss: 0.12229057593009218 \n",
      "epoch: 1565, train_loss: 0.12234596428247282 \n",
      "epoch: 1566, train_loss: 0.12225516382839746 \n",
      "epoch: 1567, train_loss: 0.1223326403013004 \n",
      "epoch: 1568, train_loss: 0.1222397585112177 \n",
      "epoch: 1569, train_loss: 0.12232012210967065 \n",
      "epoch: 1570, train_loss: 0.12223138849603161 \n",
      "epoch: 1571, train_loss: 0.12230851482272193 \n",
      "epoch: 1572, train_loss: 0.12222427078492433 \n",
      "epoch: 1573, train_loss: 0.12229774044441791 \n",
      "epoch: 1574, train_loss: 0.12221552713198766 \n",
      "epoch: 1575, train_loss: 0.1222902059710427 \n",
      "epoch: 1576, train_loss: 0.12220965744296931 \n",
      "epoch: 1577, train_loss: 0.12228332533221899 \n",
      "epoch: 1578, train_loss: 0.12220536328828706 \n",
      "epoch: 1579, train_loss: 0.12227620180686438 \n",
      "epoch: 1580, train_loss: 0.12220167283387914 \n",
      "epoch: 1581, train_loss: 0.122267646541076 \n",
      "epoch: 1582, train_loss: 0.12219702291939437 \n",
      "epoch: 1583, train_loss: 0.12225654313208298 \n",
      "epoch: 1584, train_loss: 0.12218956391077995 \n",
      "epoch: 1585, train_loss: 0.12224328375576594 \n",
      "epoch: 1586, train_loss: 0.12217762134478648 \n",
      "epoch: 1587, train_loss: 0.12223167483661766 \n",
      "epoch: 1588, train_loss: 0.12216415030105143 \n",
      "epoch: 1589, train_loss: 0.12221982071107816 \n",
      "epoch: 1590, train_loss: 0.12215444558326201 \n",
      "epoch: 1591, train_loss: 0.12220404956255032 \n",
      "epoch: 1592, train_loss: 0.12214491888265122 \n",
      "epoch: 1593, train_loss: 0.12218996526369813 \n",
      "epoch: 1594, train_loss: 0.12213472885442678 \n",
      "epoch: 1595, train_loss: 0.1221768569943424 \n",
      "epoch: 1596, train_loss: 0.12212414490658574 \n",
      "epoch: 1597, train_loss: 0.12216546173451727 \n",
      "epoch: 1598, train_loss: 0.12211323170195666 \n",
      "epoch: 1599, train_loss: 0.12215639837140893 \n",
      "epoch: 1600, train_loss: 0.12210342086466623 \n",
      "epoch: 1601, train_loss: 0.12215073857806201 \n",
      "epoch: 1602, train_loss: 0.12209863509845542 \n",
      "epoch: 1603, train_loss: 0.12214485381696472 \n",
      "epoch: 1604, train_loss: 0.12210258176290528 \n",
      "epoch: 1605, train_loss: 0.12215892037878517 \n",
      "epoch: 1606, train_loss: 0.122122586440297 \n",
      "epoch: 1607, train_loss: 0.12213959469591659 \n",
      "epoch: 1608, train_loss: 0.12210235636450396 \n",
      "epoch: 1609, train_loss: 0.12233030425668268 \n",
      "epoch: 1610, train_loss: 0.1221351321856948 \n",
      "epoch: 1611, train_loss: 0.12198972668084009 \n",
      "epoch: 1612, train_loss: 0.12205920862115996 \n",
      "epoch: 1613, train_loss: 0.12223293650580117 \n",
      "epoch: 1614, train_loss: 0.12200869219290911 \n",
      "epoch: 1615, train_loss: 0.12205205394598405 \n",
      "epoch: 1616, train_loss: 0.12186145440865434 \n",
      "epoch: 1617, train_loss: 0.12204547762915847 \n",
      "epoch: 1618, train_loss: 0.12210523735784265 \n",
      "epoch: 1619, train_loss: 0.12205894765747803 \n",
      "epoch: 1620, train_loss: 0.1221648419111827 \n",
      "epoch: 1621, train_loss: 0.12216286693593856 \n",
      "epoch: 1622, train_loss: 0.1220576628226131 \n",
      "epoch: 1623, train_loss: 0.12200983629443449 \n",
      "epoch: 1624, train_loss: 0.12206220996692341 \n",
      "epoch: 1625, train_loss: 0.12205464328192912 \n",
      "epoch: 1626, train_loss: 0.12208627351551808 \n",
      "epoch: 1627, train_loss: 0.12199261264415354 \n",
      "epoch: 1628, train_loss: 0.12208942299848681 \n",
      "epoch: 1629, train_loss: 0.1221262206628066 \n",
      "epoch: 1630, train_loss: 0.12202372346371426 \n",
      "epoch: 1631, train_loss: 0.12191148307063795 \n",
      "epoch: 1632, train_loss: 0.12199383810559594 \n",
      "epoch: 1633, train_loss: 0.12201907188223117 \n",
      "epoch: 1634, train_loss: 0.1219957373263225 \n",
      "epoch: 1635, train_loss: 0.12206050487855247 \n",
      "epoch: 1636, train_loss: 0.1219552744947705 \n",
      "epoch: 1637, train_loss: 0.12208387566239177 \n",
      "epoch: 1638, train_loss: 0.12195756936769857 \n",
      "epoch: 1639, train_loss: 0.12208173026813152 \n",
      "epoch: 1640, train_loss: 0.12195294933903653 \n",
      "epoch: 1641, train_loss: 0.1220752662877437 \n",
      "epoch: 1642, train_loss: 0.12201381346054771 \n",
      "epoch: 1643, train_loss: 0.12203442545118054 \n",
      "epoch: 1644, train_loss: 0.12203202742788341 \n",
      "epoch: 1645, train_loss: 0.12203386484511787 \n",
      "epoch: 1646, train_loss: 0.12202936637095542 \n",
      "epoch: 1647, train_loss: 0.12202616259586088 \n",
      "epoch: 1648, train_loss: 0.12200817112247085 \n",
      "epoch: 1649, train_loss: 0.12200008105123615 \n",
      "epoch: 1650, train_loss: 0.12198366101441437 \n",
      "epoch: 1651, train_loss: 0.12196564459171409 \n",
      "epoch: 1652, train_loss: 0.12206077387120005 \n",
      "epoch: 1653, train_loss: 0.12192513582164492 \n",
      "epoch: 1654, train_loss: 0.12194564849597897 \n",
      "epoch: 1655, train_loss: 0.12196358936232646 \n",
      "epoch: 1656, train_loss: 0.1219769181179148 \n",
      "epoch: 1657, train_loss: 0.12205274161450039 \n",
      "epoch: 1658, train_loss: 0.12195582603162937 \n",
      "epoch: 1659, train_loss: 0.12195547801031875 \n",
      "epoch: 1660, train_loss: 0.12196289964554008 \n",
      "epoch: 1661, train_loss: 0.12195948699476022 \n",
      "epoch: 1662, train_loss: 0.12196582893298182 \n",
      "epoch: 1663, train_loss: 0.12196147772968115 \n",
      "epoch: 1664, train_loss: 0.12196831176250228 \n",
      "epoch: 1665, train_loss: 0.12196256033875066 \n",
      "epoch: 1666, train_loss: 0.12195358179421315 \n",
      "epoch: 1667, train_loss: 0.12194574867496671 \n",
      "epoch: 1668, train_loss: 0.12193989337598787 \n",
      "epoch: 1669, train_loss: 0.12193909478546128 \n",
      "epoch: 1670, train_loss: 0.12193947099212345 \n",
      "epoch: 1671, train_loss: 0.12191494636624325 \n",
      "epoch: 1672, train_loss: 0.12190908911713239 \n",
      "epoch: 1673, train_loss: 0.12190483528725013 \n",
      "epoch: 1674, train_loss: 0.121904659645343 \n",
      "epoch: 1675, train_loss: 0.12190406950150531 \n",
      "epoch: 1676, train_loss: 0.12190178132953813 \n",
      "epoch: 1677, train_loss: 0.12189805524985345 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1678, train_loss: 0.12189392591983281 \n",
      "epoch: 1679, train_loss: 0.12188968386233183 \n",
      "epoch: 1680, train_loss: 0.12188548063326059 \n",
      "epoch: 1681, train_loss: 0.12188135763311209 \n",
      "epoch: 1682, train_loss: 0.1218773218107244 \n",
      "epoch: 1683, train_loss: 0.12187336004172838 \n",
      "epoch: 1684, train_loss: 0.12186945446903462 \n",
      "epoch: 1685, train_loss: 0.12186558751284368 \n",
      "epoch: 1686, train_loss: 0.12186174501013186 \n",
      "epoch: 1687, train_loss: 0.12185791715462324 \n",
      "epoch: 1688, train_loss: 0.12185409831211444 \n",
      "epoch: 1689, train_loss: 0.12185028584994445 \n",
      "epoch: 1690, train_loss: 0.12184647854852648 \n",
      "epoch: 1691, train_loss: 0.12184267525122974 \n",
      "epoch: 1692, train_loss: 0.12183887435070172 \n",
      "epoch: 1693, train_loss: 0.12183507442549545 \n",
      "epoch: 1694, train_loss: 0.12183127597563521 \n",
      "epoch: 1695, train_loss: 0.12182748384649217 \n",
      "epoch: 1696, train_loss: 0.12182370967932626 \n",
      "epoch: 1697, train_loss: 0.1218199736991727 \n",
      "epoch: 1698, train_loss: 0.12181630539742243 \n",
      "epoch: 1699, train_loss: 0.12181274309619088 \n",
      "epoch: 1700, train_loss: 0.12180933282890298 \n",
      "epoch: 1701, train_loss: 0.12180612729957586 \n",
      "epoch: 1702, train_loss: 0.12180318546096632 \n",
      "epoch: 1703, train_loss: 0.12180057144990523 \n",
      "epoch: 1704, train_loss: 0.12179835037058052 \n",
      "epoch: 1705, train_loss: 0.12179659482356973 \n",
      "epoch: 1706, train_loss: 0.12179556637900477 \n",
      "epoch: 1707, train_loss: 0.12176148458284633 \n",
      "epoch: 1708, train_loss: 0.12174461768143115 \n",
      "epoch: 1709, train_loss: 0.12139261035255573 \n",
      "epoch: 1710, train_loss: 0.12178935597227333 \n",
      "epoch: 1711, train_loss: 0.12181167972799258 \n",
      "epoch: 1712, train_loss: 0.12165939120889671 \n",
      "epoch: 1713, train_loss: 0.12204244686375779 \n",
      "epoch: 1714, train_loss: 0.12194621908544816 \n",
      "epoch: 1715, train_loss: 0.12162163112423888 \n",
      "epoch: 1716, train_loss: 0.12201374368406949 \n",
      "epoch: 1717, train_loss: 0.12203153873448838 \n",
      "epoch: 1718, train_loss: 0.12203502648658301 \n",
      "epoch: 1719, train_loss: 0.12202555696157456 \n",
      "epoch: 1720, train_loss: 0.12202298884008675 \n",
      "epoch: 1721, train_loss: 0.12201448234779758 \n",
      "epoch: 1722, train_loss: 0.12198347671606861 \n",
      "epoch: 1723, train_loss: 0.12194932279749995 \n",
      "epoch: 1724, train_loss: 0.12173967751255066 \n",
      "epoch: 1725, train_loss: 0.12195718580120997 \n",
      "epoch: 1726, train_loss: 0.1216521451495568 \n",
      "epoch: 1727, train_loss: 0.12156383753641159 \n",
      "epoch: 1728, train_loss: 0.12186864598910575 \n",
      "epoch: 1729, train_loss: 0.12153874669556292 \n",
      "epoch: 1730, train_loss: 0.12147433804130887 \n",
      "epoch: 1731, train_loss: 0.12176597955238007 \n",
      "epoch: 1732, train_loss: 0.12158808236317761 \n",
      "epoch: 1733, train_loss: 0.12185037535438348 \n",
      "epoch: 1734, train_loss: 0.12159644003922761 \n",
      "epoch: 1735, train_loss: 0.12161692651628421 \n",
      "epoch: 1736, train_loss: 0.12191271800029554 \n",
      "epoch: 1737, train_loss: 0.12153776978115295 \n",
      "epoch: 1738, train_loss: 0.12182768643077273 \n",
      "epoch: 1739, train_loss: 0.12152359739982652 \n",
      "epoch: 1740, train_loss: 0.12181862118916333 \n",
      "epoch: 1741, train_loss: 0.12150925031921185 \n",
      "epoch: 1742, train_loss: 0.12180739855033251 \n",
      "epoch: 1743, train_loss: 0.12149249118449244 \n",
      "epoch: 1744, train_loss: 0.12179756938346785 \n",
      "epoch: 1745, train_loss: 0.12153179950985155 \n",
      "epoch: 1746, train_loss: 0.12186567814031493 \n",
      "epoch: 1747, train_loss: 0.12184714859736417 \n",
      "epoch: 1748, train_loss: 0.12154179704373802 \n",
      "epoch: 1749, train_loss: 0.12181395309374075 \n",
      "epoch: 1750, train_loss: 0.12185984022274418 \n",
      "epoch: 1751, train_loss: 0.12151286815810143 \n",
      "epoch: 1752, train_loss: 0.12168017358992796 \n",
      "epoch: 1753, train_loss: 0.12170614091014881 \n",
      "epoch: 1754, train_loss: 0.12170602855497181 \n",
      "epoch: 1755, train_loss: 0.12171814682505877 \n",
      "epoch: 1756, train_loss: 0.12172236283286819 \n",
      "epoch: 1757, train_loss: 0.1217259404476513 \n",
      "epoch: 1758, train_loss: 0.12172629682835953 \n",
      "epoch: 1759, train_loss: 0.12172559229328618 \n",
      "epoch: 1760, train_loss: 0.12172814156479823 \n",
      "epoch: 1761, train_loss: 0.1217375594318423 \n",
      "epoch: 1762, train_loss: 0.12174465466512095 \n",
      "epoch: 1763, train_loss: 0.12177427679189172 \n",
      "epoch: 1764, train_loss: 0.12175755391696515 \n",
      "epoch: 1765, train_loss: 0.12154006760352926 \n",
      "epoch: 1766, train_loss: 0.12181814241978832 \n",
      "epoch: 1767, train_loss: 0.12160733331507168 \n",
      "epoch: 1768, train_loss: 0.12179390287791798 \n",
      "epoch: 1769, train_loss: 0.12153975105698725 \n",
      "epoch: 1770, train_loss: 0.12179474190588711 \n",
      "epoch: 1771, train_loss: 0.12158485389725536 \n",
      "epoch: 1772, train_loss: 0.12175345418111592 \n",
      "epoch: 1773, train_loss: 0.12165577112331158 \n",
      "epoch: 1774, train_loss: 0.12174172511566574 \n",
      "epoch: 1775, train_loss: 0.12168921633346516 \n",
      "epoch: 1776, train_loss: 0.12169676747071884 \n",
      "epoch: 1777, train_loss: 0.12166762488033017 \n",
      "epoch: 1778, train_loss: 0.12163432111202407 \n",
      "epoch: 1779, train_loss: 0.1216540322609573 \n",
      "epoch: 1780, train_loss: 0.12164557131619169 \n",
      "epoch: 1781, train_loss: 0.12162225128328856 \n",
      "epoch: 1782, train_loss: 0.12173321258743394 \n",
      "epoch: 1783, train_loss: 0.12161891074033047 \n",
      "epoch: 1784, train_loss: 0.12167250772748649 \n",
      "epoch: 1785, train_loss: 0.12155169227293335 \n",
      "epoch: 1786, train_loss: 0.12157993229771179 \n",
      "epoch: 1787, train_loss: 0.12160168431800697 \n",
      "epoch: 1788, train_loss: 0.12167275024395614 \n",
      "epoch: 1789, train_loss: 0.12159570899757834 \n",
      "epoch: 1790, train_loss: 0.12150792176920833 \n",
      "epoch: 1791, train_loss: 0.1214897609320329 \n",
      "epoch: 1792, train_loss: 0.12145178416445537 \n",
      "epoch: 1793, train_loss: 0.1214747207348038 \n",
      "epoch: 1794, train_loss: 0.12144982921976342 \n",
      "epoch: 1795, train_loss: 0.12146326589687681 \n",
      "epoch: 1796, train_loss: 0.12144262674042902 \n",
      "epoch: 1797, train_loss: 0.12145193626074065 \n",
      "epoch: 1798, train_loss: 0.12143274666079892 \n",
      "epoch: 1799, train_loss: 0.121440370808729 \n",
      "epoch: 1800, train_loss: 0.12142096012716927 \n",
      "epoch: 1801, train_loss: 0.12142847276583446 \n",
      "epoch: 1802, train_loss: 0.12140979775703456 \n",
      "epoch: 1803, train_loss: 0.12141763113311081 \n",
      "epoch: 1804, train_loss: 0.12140669359003955 \n",
      "epoch: 1805, train_loss: 0.12141128998329824 \n",
      "epoch: 1806, train_loss: 0.12141094659337073 \n",
      "epoch: 1807, train_loss: 0.12141019398907191 \n",
      "epoch: 1808, train_loss: 0.12141010938751101 \n",
      "epoch: 1809, train_loss: 0.12140754295369925 \n",
      "epoch: 1810, train_loss: 0.12140225940553968 \n",
      "epoch: 1811, train_loss: 0.12139753981266804 \n",
      "epoch: 1812, train_loss: 0.1213915211051039 \n",
      "epoch: 1813, train_loss: 0.12138665127862405 \n",
      "epoch: 1814, train_loss: 0.12138117588193006 \n",
      "epoch: 1815, train_loss: 0.12137621514008579 \n",
      "epoch: 1816, train_loss: 0.12137025135624895 \n",
      "epoch: 1817, train_loss: 0.12136434517145507 \n",
      "epoch: 1818, train_loss: 0.12135742774578712 \n",
      "epoch: 1819, train_loss: 0.12135092591679718 \n",
      "epoch: 1820, train_loss: 0.12134382490414668 \n",
      "epoch: 1821, train_loss: 0.12133739626062756 \n",
      "epoch: 1822, train_loss: 0.12133057760086144 \n",
      "epoch: 1823, train_loss: 0.12132439575830044 \n",
      "epoch: 1824, train_loss: 0.1213179152924886 \n",
      "epoch: 1825, train_loss: 0.12131193700798 \n",
      "epoch: 1826, train_loss: 0.12130572259487361 \n",
      "epoch: 1827, train_loss: 0.12129986981265516 \n",
      "epoch: 1828, train_loss: 0.12129383478087657 \n",
      "epoch: 1829, train_loss: 0.12128804353255222 \n",
      "epoch: 1830, train_loss: 0.12128211563554823 \n",
      "epoch: 1831, train_loss: 0.12127634100479394 \n",
      "epoch: 1832, train_loss: 0.12127046506637523 \n",
      "epoch: 1833, train_loss: 0.12126467605574275 \n",
      "epoch: 1834, train_loss: 0.1212588103647547 \n",
      "epoch: 1835, train_loss: 0.12125298427181312 \n",
      "epoch: 1836, train_loss: 0.12124709642856064 \n",
      "epoch: 1837, train_loss: 0.12124121467884175 \n",
      "epoch: 1838, train_loss: 0.12123527810661205 \n",
      "epoch: 1839, train_loss: 0.12122932342878207 \n",
      "epoch: 1840, train_loss: 0.1212233145613377 \n",
      "epoch: 1841, train_loss: 0.12121726899712267 \n",
      "epoch: 1842, train_loss: 0.12121116489801385 \n",
      "epoch: 1843, train_loss: 0.12120500815998729 \n",
      "epoch: 1844, train_loss: 0.12119878428748974 \n",
      "epoch: 1845, train_loss: 0.1211924920489938 \n",
      "epoch: 1846, train_loss: 0.12118611986249253 \n",
      "epoch: 1847, train_loss: 0.12117966161999476 \n",
      "epoch: 1848, train_loss: 0.12117310573134939 \n",
      "epoch: 1849, train_loss: 0.12116644196055035 \n",
      "epoch: 1850, train_loss: 0.12115965657228672 \n",
      "epoch: 1851, train_loss: 0.12115273481475908 \n",
      "epoch: 1852, train_loss: 0.12114565849494235 \n",
      "epoch: 1853, train_loss: 0.12113840574661927 \n",
      "epoch: 1854, train_loss: 0.12113094787605633 \n",
      "epoch: 1855, train_loss: 0.12112324510343203 \n",
      "epoch: 1856, train_loss: 0.12111524018710962 \n",
      "epoch: 1857, train_loss: 0.12110685592657662 \n",
      "epoch: 1858, train_loss: 0.12109801016167165 \n",
      "epoch: 1859, train_loss: 0.12108866080838393 \n",
      "epoch: 1860, train_loss: 0.12107885563182522 \n",
      "epoch: 1861, train_loss: 0.1210687227456714 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1862, train_loss: 0.12105839420779593 \n",
      "epoch: 1863, train_loss: 0.12104794403126995 \n",
      "epoch: 1864, train_loss: 0.1210373937805974 \n",
      "epoch: 1865, train_loss: 0.12102675740194013 \n",
      "epoch: 1866, train_loss: 0.12101607488785306 \n",
      "epoch: 1867, train_loss: 0.12100541439223246 \n",
      "epoch: 1868, train_loss: 0.12099485484108032 \n",
      "epoch: 1869, train_loss: 0.12098446836892308 \n",
      "epoch: 1870, train_loss: 0.12097431144698358 \n",
      "epoch: 1871, train_loss: 0.12096442364312697 \n",
      "epoch: 1872, train_loss: 0.12095483000709183 \n",
      "epoch: 1873, train_loss: 0.12094554420846809 \n",
      "epoch: 1874, train_loss: 0.12093657126032774 \n",
      "epoch: 1875, train_loss: 0.1209279096395835 \n",
      "epoch: 1876, train_loss: 0.12091955293462525 \n",
      "epoch: 1877, train_loss: 0.12091149116675605 \n",
      "epoch: 1878, train_loss: 0.12090371187388443 \n",
      "epoch: 1879, train_loss: 0.12089620099880405 \n",
      "epoch: 1880, train_loss: 0.12088894360345206 \n",
      "epoch: 1881, train_loss: 0.12088192442590595 \n",
      "epoch: 1882, train_loss: 0.12087512829860117 \n",
      "epoch: 1883, train_loss: 0.12086854044850227 \n",
      "epoch: 1884, train_loss: 0.12086214670062972 \n",
      "epoch: 1885, train_loss: 0.12085593360521772 \n",
      "epoch: 1886, train_loss: 0.12084988850643782 \n",
      "epoch: 1887, train_loss: 0.12084399956772078 \n",
      "epoch: 1888, train_loss: 0.12083825576575533 \n",
      "epoch: 1889, train_loss: 0.12083264686253534 \n",
      "epoch: 1890, train_loss: 0.12082716336251842 \n",
      "epoch: 1891, train_loss: 0.12082179646006387 \n",
      "epoch: 1892, train_loss: 0.12081653798085232 \n",
      "epoch: 1893, train_loss: 0.12081138031985963 \n",
      "epoch: 1894, train_loss: 0.12080631637766732 \n",
      "epoch: 1895, train_loss: 0.12080133949637302 \n",
      "epoch: 1896, train_loss: 0.12079644339611557 \n",
      "epoch: 1897, train_loss: 0.12079162211326251 \n",
      "epoch: 1898, train_loss: 0.12078686994160004 \n",
      "epoch: 1899, train_loss: 0.1207821813784263 \n",
      "epoch: 1900, train_loss: 0.12077755107816913 \n",
      "epoch: 1901, train_loss: 0.12077297381682463 \n",
      "epoch: 1902, train_loss: 0.12076844447080984 \n",
      "epoch: 1903, train_loss: 0.12076395801330071 \n",
      "epoch: 1904, train_loss: 0.12075950952937287 \n",
      "epoch: 1905, train_loss: 0.1207550942482505 \n",
      "epoch: 1906, train_loss: 0.12075070758718205 \n",
      "epoch: 1907, train_loss: 0.1207463451980705 \n",
      "epoch: 1908, train_loss: 0.12074200300641562 \n",
      "epoch: 1909, train_loss: 0.12073767723329575 \n",
      "epoch: 1910, train_loss: 0.12073336439492391 \n",
      "epoch: 1911, train_loss: 0.12072906127940382 \n",
      "epoch: 1912, train_loss: 0.12072476490492226 \n",
      "epoch: 1913, train_loss: 0.12072047246639461 \n",
      "epoch: 1914, train_loss: 0.12071618127814376 \n",
      "epoch: 1915, train_loss: 0.12071188871895733 \n",
      "epoch: 1916, train_loss: 0.12070759218369766 \n",
      "epoch: 1917, train_loss: 0.12070328904320135 \n",
      "epoch: 1918, train_loss: 0.12069897661214399 \n",
      "epoch: 1919, train_loss: 0.12069465212338036 \n",
      "epoch: 1920, train_loss: 0.1206903127077955 \n",
      "epoch: 1921, train_loss: 0.12068595538227476 \n",
      "epoch: 1922, train_loss: 0.12068157705702921 \n",
      "epoch: 1923, train_loss: 0.12067717458898615 \n",
      "epoch: 1924, train_loss: 0.12067274492892935 \n",
      "epoch: 1925, train_loss: 0.12066828542386628 \n",
      "epoch: 1926, train_loss: 0.1206637943074718 \n",
      "epoch: 1927, train_loss: 0.12065927128795662 \n",
      "epoch: 1928, train_loss: 0.12065471793023685 \n",
      "epoch: 1929, train_loss: 0.12065013744076544 \n",
      "epoch: 1930, train_loss: 0.12064553385636123 \n",
      "epoch: 1931, train_loss: 0.12064091140921811 \n",
      "epoch: 1932, train_loss: 0.12063627513943824 \n",
      "epoch: 1933, train_loss: 0.12063163318275873 \n",
      "epoch: 1934, train_loss: 0.12062699978343532 \n",
      "epoch: 1935, train_loss: 0.12062239556265669 \n",
      "epoch: 1936, train_loss: 0.12061784053378448 \n",
      "epoch: 1937, train_loss: 0.12061334399168484 \n",
      "epoch: 1938, train_loss: 0.12060890381707647 \n",
      "epoch: 1939, train_loss: 0.12060451440243247 \n",
      "epoch: 1940, train_loss: 0.12060017202039593 \n",
      "epoch: 1941, train_loss: 0.12059587469396002 \n",
      "epoch: 1942, train_loss: 0.12059161929597732 \n",
      "epoch: 1943, train_loss: 0.12058739840635138 \n",
      "epoch: 1944, train_loss: 0.12058319866901816 \n",
      "epoch: 1945, train_loss: 0.12057900265692308 \n",
      "epoch: 1946, train_loss: 0.12057479874509361 \n",
      "epoch: 1947, train_loss: 0.1205706077443722 \n",
      "epoch: 1948, train_loss: 0.12056653147135109 \n",
      "epoch: 1949, train_loss: 0.12056280844952784 \n",
      "epoch: 1950, train_loss: 0.12055980680242877 \n",
      "epoch: 1951, train_loss: 0.12055822608266421 \n",
      "epoch: 1952, train_loss: 0.12057582659020785 \n",
      "epoch: 1953, train_loss: 0.12056896581666109 \n",
      "epoch: 1954, train_loss: 0.12057440293713526 \n",
      "epoch: 1955, train_loss: 0.12057726268278868 \n",
      "epoch: 1956, train_loss: 0.12054634549437987 \n",
      "epoch: 1957, train_loss: 0.12055943936544758 \n",
      "epoch: 1958, train_loss: 0.1205683703700937 \n",
      "epoch: 1959, train_loss: 0.12053889102466016 \n",
      "epoch: 1960, train_loss: 0.12055614127357786 \n",
      "epoch: 1961, train_loss: 0.12055769418022028 \n",
      "epoch: 1962, train_loss: 0.12054875377210783 \n",
      "epoch: 1963, train_loss: 0.12054361896435963 \n",
      "epoch: 1964, train_loss: 0.1205245733131724 \n",
      "epoch: 1965, train_loss: 0.12053770588953461 \n",
      "epoch: 1966, train_loss: 0.12054440435621044 \n",
      "epoch: 1967, train_loss: 0.12053206332557191 \n",
      "epoch: 1968, train_loss: 0.12052654353645176 \n",
      "epoch: 1969, train_loss: 0.12052721449856385 \n",
      "epoch: 1970, train_loss: 0.12052586714848788 \n",
      "epoch: 1971, train_loss: 0.12052296810747307 \n",
      "epoch: 1972, train_loss: 0.12052292823603959 \n",
      "epoch: 1973, train_loss: 0.12052002700108967 \n",
      "epoch: 1974, train_loss: 0.12051289693931669 \n",
      "epoch: 1975, train_loss: 0.12051456074319108 \n",
      "epoch: 1976, train_loss: 0.12051796359831174 \n",
      "epoch: 1977, train_loss: 0.12043461018137447 \n",
      "epoch: 1978, train_loss: 0.12047833944227725 \n",
      "epoch: 1979, train_loss: 0.1204320067673512 \n",
      "epoch: 1980, train_loss: 0.120405109055971 \n",
      "epoch: 1981, train_loss: 0.12049501463987802 \n",
      "epoch: 1982, train_loss: 0.12053340874846619 \n",
      "epoch: 1983, train_loss: 0.12042841101405816 \n",
      "epoch: 1984, train_loss: 0.12041667942299338 \n",
      "epoch: 1985, train_loss: 0.12041531412692415 \n",
      "epoch: 1986, train_loss: 0.12043720471208043 \n",
      "epoch: 1987, train_loss: 0.12044503328882908 \n",
      "epoch: 1988, train_loss: 0.12043274070173858 \n",
      "epoch: 1989, train_loss: 0.12043993757149635 \n",
      "epoch: 1990, train_loss: 0.12043459660601726 \n",
      "epoch: 1991, train_loss: 0.12044220085797314 \n",
      "epoch: 1992, train_loss: 0.12043980996182077 \n",
      "epoch: 1993, train_loss: 0.12044225200501876 \n",
      "epoch: 1994, train_loss: 0.12045366834401601 \n",
      "epoch: 1995, train_loss: 0.12046397367379535 \n",
      "epoch: 1996, train_loss: 0.12035635178481408 \n",
      "epoch: 1997, train_loss: 0.12032177209474199 \n",
      "epoch: 1998, train_loss: 0.1203262878723902 \n",
      "epoch: 1999, train_loss: 0.12031390184949257 \n",
      "epoch: 2000, train_loss: 0.1203100736738423 \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#라이브러리 불러오기\n",
    "import numpy as np               #넘파이 불러오기\n",
    "import matplotlib.pyplot as plt  #매트플롯립 불러오기\n",
    "\n",
    "#시그모이드 함수 정의하기\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "#데이터 세트 불러오기\n",
    "data_set=np.loadtxt('training.txt')\n",
    "\n",
    "#데이터 세트 랜덤으로 섞기\n",
    "np.random.shuffle(data_set) \n",
    "\n",
    "#데이터 세트 불러와서 변수지정하기\n",
    "x=data_set[:,0:2]  # 행 1000, 열 2(0~1번째) #(1000,2)\n",
    "t=data_set[:,2:3]  # 행 1000, 열 1(2번째) #(1000,1)\n",
    "\n",
    "#layer별 크기 부여하기(bias=0)\n",
    "K=2\n",
    "N=8\n",
    "M=1\n",
    "\n",
    "#learning_rate 입력\n",
    "lr=0.01\n",
    "\n",
    "#가중치 함수 랜덤 데이터 입력하기\n",
    "w=np.random.random((K,N)) #input~hidden layer          # (2, 8)\n",
    "wp=np.random.random((N,M)) #hidden layer~output layer  # (8, 1)\n",
    "\n",
    "#에포크 크기 정하기\n",
    "num_epoch=2000\n",
    "\n",
    "#forward\n",
    "EPOCH=[]\n",
    "LOSS=[]\n",
    "for epoch in range(num_epoch): \n",
    "    E = 0\n",
    "    EPOCH.append(epoch+1)    #EPOCH 리스트에 epoch를 실행할때마다 추가하기\n",
    "    for e in range(0, 1000):\n",
    "        u=np.dot(x[e], w)    #(8,)\n",
    "        h=sigmoid(u)         #(8,)\n",
    "        up=np.dot(h, wp)    # (1,)\n",
    "        y=sigmoid(up)       # (1,)\n",
    "\n",
    "    #loss function\n",
    "        E_temp=(1/2)*(y-t[e])**2 #(1,)\n",
    "        E += E_temp[0]           #E값은 1000번 반복하여 실시한 최종값\n",
    "         \n",
    "    #back propagation\n",
    "        EIP = (y-t[e])*y*(1-y) #(1,)     # wp=(8,1)\n",
    "        EI = np.sum(EIP*wp)*h*(1-h)      #(8,)       \n",
    "        EI = np.expand_dims(EI, axis=1)  #(8,1)\n",
    "        \n",
    "        dwp = EIP * h            # (8, 1)\n",
    "        dw = (EI*x[e]).T        # (2,8) = ((8,1) * (1,2)).T #행렬을 맞추기위해 .T실시\n",
    "        \n",
    "        # weight update\n",
    "        wp = wp - lr*dwp      \n",
    "        w = w - lr*dw    \n",
    "    LOSS.append(E/1000)         # LOSS 리스트에 E/1000을 실행할때마다 추가하기\n",
    "   \n",
    "      \n",
    "    print(\"epoch: {}, train_loss: {} \" .format(epoch+1, E/1000)) #잘나오는지 확인\n",
    "                   \n",
    "    \n",
    "#그래프 만들기\n",
    "\n",
    "plt.plot(EPOCH,LOSS)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()   \n",
    "\n",
    "#실험의 결론\n",
    "#1)epoch의 크기가 클수록 LOSS의 값이 최소화되는 경향이 있다.\n",
    "#2)epoch의 크기가 너무 적으면(약 500이하) epoch max값이 반드시 LOSS 최소값이 아니다.\n",
    "   #→ 산포가 있기 때문에, epoch가 크면 클수록 LOSS 값이 최소화되는 경향 재확인함    \n",
    "\n",
    "#느낀점\n",
    "#1)실험을 통해서, FORWARD와 BACK PROPAGATION을 통한 가중치(W,WP) 변화를 눈으로 확인함\n",
    "#2)실험을 통해서, 행렬의 내적시 행/열의 숫자를 맞추는 것이 중요하다는 것을 깨달음\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
